{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from konlpy.tag import Komoran\n",
    "import networkx\n",
    "import re\n",
    "from shutil import rmtree\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/TestDir/sample_articles\"\n",
    "PREPROCESSED_PATH = os.path.join(BASE_DIR,\"Preprocessed-Data\")\n",
    "PRETTY_PATH = os.path.join(BASE_DIR,\"Pretty-Data\")\n",
    "ORIGIN_PATH = os.path.join(BASE_DIR,\"Origin-Data\")\n",
    "SUMMARY_PATH = os.path.join(BASE_DIR,\"Summary-Data\")\n",
    "SWORDS_PATH = os.path.join(BASE_DIR, \"StopWordList.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawTextReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile(\"/n\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in ch:\n",
    "                yield s\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, originSentenceIter, procSentenceIter):\n",
    "        self.originSents = list(filter(None, originSentenceIter))\n",
    "        self.procSents = list(filter(None, procSentenceIter))\n",
    "\n",
    "    def getOriginSet(self):\n",
    "        return self.originSents\n",
    "\n",
    "    def getSentsZip(self):\n",
    "        return zip(self.originSents, self.procSents)\n",
    "\n",
    "\n",
    "class TextRank:\n",
    "    def __init__(self, **kargs):\n",
    "        self.graph = None\n",
    "        self.coef = kargs.get('coef', 1.0)\n",
    "        self.threshold = kargs.get('threshold', 0.005)\n",
    "        self.dictCount = {}\n",
    "        self.dictBiCount = {}\n",
    "\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        self.tfidf_matrix = {}\n",
    "\n",
    "    def loadSents(self, document, tokenizer):\n",
    "\n",
    "        sentSet = []\n",
    "        \n",
    "        for origin, proc in document.getSentsZip():\n",
    "            tagged = set(filter(None, tokenizer(proc)))\n",
    "            if len(tagged) < 2: continue\n",
    "            self.dictCount[len(self.dictCount)] = origin\n",
    "            sentSet.append(tagged)\n",
    "        \n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(document.getOriginSet())\n",
    "\n",
    "        sents_distances = (self.tfidf_matrix * self.tfidf_matrix.T)\n",
    "        sents_distances_matrix = sents_distances.toarray()\n",
    "\n",
    "        for i in range(len(self.dictCount)):\n",
    "            for j in range(i + 1, len(self.dictCount)):\n",
    "                similarity = sents_distances_matrix[i, j]\n",
    "\n",
    "                if similarity < self.threshold: continue\n",
    "                self.dictBiCount[i, j] = similarity\n",
    "\n",
    "    def build(self):\n",
    "        self.graph = networkx.Graph()\n",
    "        self.graph.add_nodes_from(self.dictCount.keys())\n",
    "        for (a, b), n in self.dictBiCount.items():\n",
    "            self.graph.add_edge(a, b, weight=n * self.coef + (1 - self.coef))\n",
    "\n",
    "    def rank(self):\n",
    "        return networkx.pagerank(self.graph, weight='weight')\n",
    "\n",
    "    def summarize(self, ratio=0.333):\n",
    "        r = self.rank()\n",
    "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r) * ratio)]\n",
    "        return ' '.join(map(lambda k: self.dictCount[k], sorted(ks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    import errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def del_folder(path):\n",
    "    try:\n",
    "        rmtree(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTextFile(baseDir, media, filename, sentence):\n",
    "\n",
    "    mkdir_p(os.path.join(baseDir, media))\n",
    "    save_path = os.path.join(os.path.join(baseDir, media), filename)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 MBN\n",
      "1246.txt\n",
      "1247.txt\n",
      "1248.txt\n",
      "1249.txt\n",
      "1353.txt\n",
      "1354.txt\n",
      "1355.txt\n",
      "1356.txt\n",
      "1357.txt\n",
      "1358.txt\n",
      "1359.txt\n",
      "136.txt\n",
      "1360.txt\n",
      "1361.txt\n",
      "1362.txt\n",
      "1363.txt\n",
      "1364.txt\n",
      "1365.txt\n",
      "1366.txt\n",
      "1367.txt\n",
      "1368.txt\n",
      "1369.txt\n",
      "137.txt\n",
      "1370.txt\n",
      "1371.txt\n",
      "1372.txt\n",
      "1373.txt\n",
      "1374.txt\n",
      "1375.txt\n",
      "1376.txt\n",
      "1377.txt\n",
      "1378.txt\n",
      "1379.txt\n",
      "138.txt\n",
      "1380.txt\n",
      "1381.txt\n",
      "1382.txt\n",
      "1383.txt\n",
      "1384.txt\n",
      "1385.txt\n",
      "1386.txt\n",
      "1387.txt\n",
      "1388.txt\n",
      "1389.txt\n",
      "139.txt\n",
      "1390.txt\n",
      "1391.txt\n",
      "1392.txt\n",
      "1393.txt\n",
      "1394.txt\n",
      "1395.txt\n",
      "1396.txt\n",
      "1397.txt\n",
      "1398.txt\n",
      "1399.txt\n",
      "14.txt\n",
      "140.txt\n",
      "1400.txt\n",
      "1401.txt\n",
      "1402.txt\n",
      "1403.txt\n",
      "1404.txt\n",
      "1405.txt\n",
      "1406.txt\n",
      "1407.txt\n",
      "1408.txt\n",
      "1409.txt\n",
      "141.txt\n",
      "1410.txt\n",
      "1411.txt\n",
      "1412.txt\n",
      "1413.txt\n",
      "1414.txt\n",
      "1415.txt\n",
      "1416.txt\n",
      "1417.txt\n",
      "1418.txt\n",
      "1419.txt\n",
      "142.txt\n",
      "1420.txt\n",
      "1421.txt\n",
      "1422.txt\n",
      "1423.txt\n",
      "1424.txt\n",
      "1425.txt\n",
      "1426.txt\n",
      "1427.txt\n",
      "1428.txt\n",
      "1429.txt\n",
      "143.txt\n",
      "1430.txt\n",
      "1431.txt\n",
      "1432.txt\n",
      "1433.txt\n",
      "1434.txt\n",
      "1435.txt\n",
      "1436.txt\n",
      "1437.txt\n",
      "1438.txt\n",
      "1439.txt\n",
      "144.txt\n",
      "1440.txt\n",
      "1441.txt\n",
      "1442.txt\n",
      "1443.txt\n",
      "1444.txt\n",
      "1445.txt\n",
      "1446.txt\n",
      "1447.txt\n",
      "1448.txt\n",
      "1449.txt\n",
      "145.txt\n",
      "1450.txt\n",
      "1468.txt\n",
      "1469.txt\n",
      "147.txt\n",
      "1470.txt\n",
      "1471.txt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    del_folder(SUMMARY_PATH)\n",
    "    media_list = os.listdir(PREPROCESSED_PATH)\n",
    "\n",
    "    for media_idx, media in enumerate(media_list) :\n",
    "\n",
    "        origin_article_list = os.listdir(os.path.join(PRETTY_PATH, media))\n",
    "        proc_article_list = os.listdir(os.path.join(PREPROCESSED_PATH, media))\n",
    "\n",
    "        print(media_idx, media)\n",
    "        for article_name in origin_article_list :\n",
    "\n",
    "            origin_article_path = os.path.join(os.path.join(PRETTY_PATH, media), article_name)\n",
    "            proc_article_path = os.path.join(os.path.join(PREPROCESSED_PATH, media), article_name)\n",
    "            \n",
    "            try :\n",
    "                tr = TextRank()\n",
    "                tagger = Komoran()\n",
    "                \n",
    "                tr.loadSents(Document(RawTextReader(origin_article_path), RawTextReader(proc_article_path)),\n",
    "                         lambda sent: filter(lambda x: x[1] in ('NNG', 'NNP', 'VV', 'VA'),\n",
    "                                             tagger.pos(sent)))\n",
    "                tr.build()\n",
    "                ranks = tr.rank()\n",
    "            \n",
    "                #for k in sorted(ranks, key=ranks.get, reverse=True)[:100]:\n",
    "                #    print(\"\\t\".join([str(k), str(ranks[k]), str(tr.dictCount[k])]))\n",
    "                #summary = tr.summarize(0.2)\n",
    "            \n",
    "                rank_order = sorted(ranks, key=ranks.get, reverse=True)[:100]\n",
    "                summary_line = str(tr.dictCount[rank_order[0]])\n",
    "            \n",
    "                saveTextFile(SUMMARY_PATH, media, article_name, summary_line)\n",
    "                \n",
    "            except Exception:\n",
    "                print(article_name)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
