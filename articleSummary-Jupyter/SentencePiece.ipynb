{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b1YFB2s2sBBN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from glob import iglob\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8tB4In3YsLDv"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/ksb/\"\n",
    "DATA_BASE_DIR = os.path.join(BASE_DIR, \"articles\")\n",
    "\n",
    "ORIGIN_PATH = os.path.join(DATA_BASE_DIR,\"Origin-Data\")\n",
    "PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Preprocessed-Data\")\n",
    "TITLE_PREPROCESSED_PATH = os.path.join(BASE_DIR,\"Title-Preprocessed-Data\")\n",
    "\n",
    "PRETTY_PATH = os.path.join(DATA_BASE_DIR,\"Pretty-Data\")\n",
    "SUMMARY_PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Summary-Preprocessed-Data\")\n",
    "SWORDS_PATH = os.path.join(DATA_BASE_DIR, \"StopWordList.txt\")\n",
    "MODEL_PATH = \"Word-Encoding-Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vgNasGwPsQ02"
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    import errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmd(filename, pad_id, bos_id, eos_id, unk_id, prefix, vocab_size, character_coverage, model_type):\n",
    "    templates= '--input={} \\\n",
    "    --pad_id={} \\\n",
    "    --bos_id={} \\\n",
    "    --eos_id={} \\\n",
    "    --unk_id={} \\\n",
    "    --model_prefix={} \\\n",
    "    --vocab_size={} \\\n",
    "    --character_coverage={} \\\n",
    "    --model_type={}'\n",
    "    \n",
    "    cmd = templates.format(file_name,\n",
    "                pad_id,\n",
    "                bos_id,\n",
    "                eos_id,\n",
    "                unk_id,\n",
    "                prefix,\n",
    "                vocab_size,\n",
    "                character_coverage,\n",
    "                model_type)\n",
    "    return cmd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(basepath):\n",
    "    result = []\n",
    "\n",
    "    for idx, proc_article_path in enumerate(iglob(os.path.join(basepath, '**.csv'), recursive=False)):\n",
    "    \n",
    "        f_proc= open(proc_article_path, 'r', newline=\"\\n\", encoding=\"utf-8\")\n",
    "        for [idx, title, contents] in csv.reader(f_proc):\n",
    "            if contents is '': continue\n",
    "\n",
    "            cont_list = contents.split(\"\\t\")\n",
    "            result.append('\\n'.join(cont_list))\n",
    "        f_proc.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LTzu0QI3sRCQ"
   },
   "outputs": [],
   "source": [
    "pad_id=0  \n",
    "vocab_size = 70000 \n",
    "bos_id=1\n",
    "eos_id=2\n",
    "unk_id=3\n",
    "character_coverage = 1.0\n",
    "model_type ='word' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(MODEL_PATH, \"Headline_SentencePiece_train.txt\")\n",
    "\n",
    "headline_src_text = get_text(TITLE_PREPROCESSED_PATH)\n",
    "headline_tar_text = get_text(SUMMARY_PREPROCESSED_PATH) # 원래 Generated Summary \n",
    "\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(headline_src_text + headline_tar_text))\n",
    "\n",
    "model_num = len(list(iglob(os.path.join(MODEL_PATH, 'spm-headline-*.vocab'), recursive=False)))\n",
    "prefix = os.path.join(MODEL_PATH, 'spm-headline-{}'.format(model_num)) \n",
    "\n",
    "headline_cmd = get_cmd(file_name, pad_id, bos_id,\n",
    "                       eos_id, unk_id, prefix, vocab_size, character_coverage, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_t1EbCU_sQ3H"
   },
   "outputs": [],
   "source": [
    "file_name = os.path.join(MODEL_PATH, \"SentencePiece_train_src.txt\")\n",
    "\n",
    "src_text = get_text(PREPROCESSED_PATH)\n",
    "\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(src_text))\n",
    "\n",
    "model_num = len(list(iglob(os.path.join(MODEL_PATH, 'spm-input-*.vocab'), recursive=False)))\n",
    "prefix = os.path.join(MODEL_PATH, 'spm-input-{}'.format(model_num))\n",
    "\n",
    "src_cmd = get_cmd(file_name, pad_id, bos_id,\n",
    "              eos_id, unk_id, prefix, vocab_size, character_coverage, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(MODEL_PATH, \"SentencePiece_train_tar.txt\")\n",
    "\n",
    "tar_text = get_text(SUMMARY_PREPROCESSED_PATH)\n",
    "\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(tar_text))\n",
    "\n",
    "model_num = len(list(iglob(os.path.join(MODEL_PATH, 'spm-summary-*.vocab'), recursive=False)))\n",
    "prefix = os.path.join(MODEL_PATH, 'spm-summary-{}'.format(model_num))\n",
    "\n",
    "tar_cmd = get_cmd(file_name, pad_id, bos_id,\n",
    "              eos_id, unk_id, prefix, vocab_size, character_coverage, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=Word-Encoding-Model/SentencePiece_train_src.txt     --pad_id=0     --bos_id=1     --eos_id=2     --unk_id=3     --model_prefix=Word-Encoding-Model/spm-input-0     --vocab_size=70000     --character_coverage=1.0     --model_type=word\n",
      "--input=Word-Encoding-Model/SentencePiece_train_tar.txt     --pad_id=0     --bos_id=1     --eos_id=2     --unk_id=3     --model_prefix=Word-Encoding-Model/spm-summary-3     --vocab_size=70000     --character_coverage=1.0     --model_type=word\n",
      "--input=Word-Encoding-Model/Headline_SentencePiece_train.txt     --pad_id=0     --bos_id=1     --eos_id=2     --unk_id=3     --model_prefix=Word-Encoding-Model/spm-headline-2     --vocab_size=70000     --character_coverage=1.0     --model_type=word\n"
     ]
    }
   ],
   "source": [
    "print(src_cmd)\n",
    "print(tar_cmd)\n",
    "print(headline_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1fycvGvktw2D"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train(src_cmd)\n",
    "spm.SentencePieceTrainer.Train(tar_cmd)\n",
    "spm.SentencePieceTrainer.Train(headline_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SentencePiece.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
