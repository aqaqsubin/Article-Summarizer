{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentencePiece.ipynb의 사본",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsrDFc_hqGcs",
        "outputId": "d0db89ff-9992-4f4a-900a-c557b40c8a26"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1YFB2s2sBBN"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import csv\r\n",
        "from glob import iglob\r\n",
        "from pathlib import Path\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIoQItMGsE75",
        "outputId": "a79434ed-d4f9-4f44-c623-96009506e458"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tB4In3YsLDv"
      },
      "source": [
        "BASE_DIR = \"/content/gdrive/My Drive/Colab Notebooks/ETRI_Article_Summarizer/\"\r\n",
        "DATA_BASE_DIR = os.path.join(BASE_DIR, \"articles\")\r\n",
        "\r\n",
        "ORIGIN_PATH = os.path.join(DATA_BASE_DIR,\"Origin-Data\")\r\n",
        "PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Preprocessed-Data\")\r\n",
        "PRETTY_PATH = os.path.join(DATA_BASE_DIR,\"Pretty-Data\")\r\n",
        "SUMMARY_PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Summary-Preprocessed-Data\")\r\n",
        "SWORDS_PATH = os.path.join(DATA_BASE_DIR, \"StopWordList.txt\")\r\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"Word-Embedding-Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STpxEgQtsPEw"
      },
      "source": [
        "MIN_COUNT = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgNasGwPsQ02"
      },
      "source": [
        "def mkdir_p(path):\r\n",
        "    import errno\r\n",
        "    try:\r\n",
        "        os.makedirs(path)\r\n",
        "    except OSError as exc:\r\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\r\n",
        "            pass\r\n",
        "        else:\r\n",
        "            raise\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t1EbCU_sQ3H"
      },
      "source": [
        "file_name = \"SentencePiece_train.txt\"\r\n",
        "result = []\r\n",
        "\r\n",
        "for idx, proc_article_path in enumerate(iglob(os.path.join(PREPROCESSED_PATH, '**.csv'), recursive=False)):\r\n",
        "    \r\n",
        "    f_proc= open(proc_article_path, 'r', newline=\"\\n\", encoding=\"utf-8\")\r\n",
        "    for [idx, title, contents] in csv.reader(f_proc):\r\n",
        "        if contents is '': continue\r\n",
        "\r\n",
        "        cont_list = contents.split(\"\\t\")\r\n",
        "        result.append('\\n'.join(cont_list))\r\n",
        "    f_proc.close()\r\n",
        "\r\n",
        "with open(file_name, 'w', encoding='utf-8') as f:\r\n",
        "    f.write('\\n'.join(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLrzCKgjsQ88"
      },
      "source": [
        "templates= '--input={} \\\r\n",
        "--pad_id={} \\\r\n",
        "--bos_id={} \\\r\n",
        "--eos_id={} \\\r\n",
        "--unk_id={} \\\r\n",
        "--model_prefix={} \\\r\n",
        "--vocab_size={} \\\r\n",
        "--character_coverage={} \\\r\n",
        "--model_type={}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTzu0QI3sRCQ"
      },
      "source": [
        "pad_id=0  #<pad> token을 0으로 설정\r\n",
        "vocab_size = 70000 \r\n",
        "model_num = len(list(iglob('**.vocab', recursive=False)))\r\n",
        "prefix = 'spm-{}'.format(model_num) \r\n",
        "bos_id=1\r\n",
        "eos_id=2\r\n",
        "unk_id=3\r\n",
        "character_coverage = 1.0\r\n",
        "model_type ='word' # Choose from unigram (default), bpe, char, or word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kdnRANEysRE8",
        "outputId": "bb2ce258-2d93-47ff-fd6c-2bfbc45adac2"
      },
      "source": [
        "cmd = templates.format(file_name,\r\n",
        "                pad_id,\r\n",
        "                bos_id,\r\n",
        "                eos_id,\r\n",
        "                unk_id,\r\n",
        "                prefix,\r\n",
        "                vocab_size,\r\n",
        "                character_coverage,\r\n",
        "                model_type)\r\n",
        "cmd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'--input=SentencePiece_train.txt --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --model_prefix=spm-2 --vocab_size=70000 --character_coverage=1.0 --model_type=word'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fycvGvktw2D"
      },
      "source": [
        "import sentencepiece as spm\r\n",
        "spm.SentencePieceTrainer.Train(cmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nU7gZxUuF34"
      },
      "source": [
        "class IntegerEncoder:\r\n",
        "    def __init__(self, filepaths, options):\r\n",
        "        self.filepaths = filepaths\r\n",
        "        \r\n",
        "        self.model = options['model-type']\r\n",
        "        self.inv_wv = options['inv_wv']\r\n",
        "        self.corpus = options['corpus']\r\n",
        "        self.sp = options['spm']\r\n",
        "    \r\n",
        "    def __get_token_matrix(self):\r\n",
        "        token_list =[]\r\n",
        "        \r\n",
        "        for path in self.filepaths:\r\n",
        "            f = open(path, 'r', newline=\"\\n\", encoding=\"utf-8\")\r\n",
        "            \r\n",
        "            for [_, title, contents] in csv.reader(f):\r\n",
        "                content = contents.split(\"\\t\")\r\n",
        "                vec = [token for sent in content for token in sent.split()]\r\n",
        "\r\n",
        "                token_list.append(np.array(vec))\r\n",
        "                \r\n",
        "            f.close()\r\n",
        "\r\n",
        "        return token_list\r\n",
        "\r\n",
        "    def __get_line_list(self):\r\n",
        "        line_list =[]\r\n",
        "        \r\n",
        "        for path in self.filepaths:\r\n",
        "            f = open(path, 'r', newline=\"\\n\", encoding=\"utf-8\")\r\n",
        "            \r\n",
        "            for [_, title, contents] in csv.reader(f):\r\n",
        "                content = contents.split(\"\\t\")\r\n",
        "                line_list.append(' '.join(content))\r\n",
        "                \r\n",
        "            f.close()\r\n",
        "\r\n",
        "        return line_list\r\n",
        "    \r\n",
        "    def __glove_encoding(self, token_list):\r\n",
        "        return list(map(lambda line: [self.corpus.dictionary[token] for token in line \r\n",
        "                                      if token in self.corpus.dictionary], token_list))\r\n",
        "        \r\n",
        "    def __sentencepiece_encoding(self, token_list):\r\n",
        "        print(token_list)\r\n",
        "        return list(map(lambda line: self.sp.EncodeAsIds(line), token_list))\r\n",
        "    \r\n",
        "    def __word2vec_encoding(self, token_list):\r\n",
        "        return list(map(lambda line: [self.inv_wv[token] for token in line\r\n",
        "                                     if token in self.inv_wv], token_list))  \r\n",
        "    \r\n",
        "    def encoder(self):\r\n",
        "\r\n",
        "        token_list = self.__get_token_matrix()\r\n",
        "        if self.model is 'GloVe':\r\n",
        "            encoding_vec_list = self.__glove_encoding(token_list) \r\n",
        "        elif self.model is 'Word2Vec' :\r\n",
        "            encoding_vec_list = self.__word2vec_encoding(token_list)\r\n",
        "        else:\r\n",
        "            encoding_vec_list = self.__sentencepiece_encoding(self.__get_line_list())\r\n",
        "        \r\n",
        "        return encoding_vec_list   \r\n",
        "    \r\n",
        "class Padding:\r\n",
        "    def __init__(self, max_len = None):\r\n",
        "        self.max_len = max_len\r\n",
        "    \r\n",
        "    def padding(self, vec_list):\r\n",
        "        vec_matrix = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "            vec_list, maxlen=self.max_len, padding='post', value=\"\", dtype='str')\r\n",
        "        \r\n",
        "        return vec_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K17R-tDv8JEA",
        "outputId": "ddce4e4f-d9d8-4e1d-9cb4-47bc4792dff3"
      },
      "source": [
        "sp = spm.SentencePieceProcessor()\r\n",
        "model_num = len(list(iglob('**.vocab', recursive=False))) -1\r\n",
        "sp.Load('spm-{}.model'.format(model_num))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnN7UfIS8Ke_",
        "outputId": "4a716484-30ba-40a1-9acd-f65d69b70707"
      },
      "source": [
        "options = {\r\n",
        "    'model-type' : 'Sentence-Piece',\r\n",
        "    'inv_wv' : None,\r\n",
        "    'corpus' : None,\r\n",
        "    'spm' : sp\r\n",
        "}\r\n",
        "input_encoded_list = IntegerEncoder(options=options, filepaths=list(iglob(os.path.join(PREPROCESSED_PATH, '**.csv'), recursive=False))).encoder()\r\n",
        "output_encoded_list = IntegerEncoder(options=options, filepaths=list(iglob(os.path.join(SUMMARY_PREPROCESSED_PATH, '**.csv'), recursive=False))).encoder()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-fEToo28M0F",
        "outputId": "799b755d-1915-4cad-bfe7-385cce8c6ba2"
      },
      "source": [
        "get_max_length = lambda x : np.max([len(line) for line in x])\r\n",
        "\r\n",
        "MAX_LEN = get_max_length(input_encoded_list)\r\n",
        "MAX_LEN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJrDyAbe8XGz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}