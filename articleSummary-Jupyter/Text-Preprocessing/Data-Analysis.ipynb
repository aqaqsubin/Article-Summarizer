{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11031,
     "status": "ok",
     "timestamp": 1610755022118,
     "user": {
      "displayName": "김수빈",
      "photoUrl": "",
      "userId": "17754099482493494628"
     },
     "user_tz": -540
    },
    "id": "X_cwe5iCxSDp",
    "outputId": "ae645a99-4108-4327-fd39-02f261a0ce43"
   },
   "outputs": [],
   "source": [
    "# _*_ coding: utf-8 _*_\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from functools import reduce\n",
    "import import_ipynb\n",
    "\n",
    "from CommonModule.Handle_Dir import mkdir_p, del_folder\n",
    "from CommonModule.ArticleHandler import Article, ArticleReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30350,
     "status": "ok",
     "timestamp": 1610755041467,
     "user": {
      "displayName": "김수빈",
      "photoUrl": "",
      "userId": "17754099482493494628"
     },
     "user_tz": -540
    },
    "id": "sxJDqNbWxam3"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/ksb/articles\"\n",
    "ORIGIN_PATH = os.path.join(BASE_DIR,\"Origin-Data\")\n",
    "PREPROCESSED_PATH = os.path.join(BASE_DIR,\"Preprocessed-Data\")\n",
    "PRETTY_PATH = os.path.join(BASE_DIR,\"Pretty-Data\")\n",
    "SWORDS_PATH = os.path.join(BASE_DIR, \"StopWordList.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_media_name(filepath):\n",
    "    filename = filepath.split(os.sep)[-1]\n",
    "    return filename.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16841,
     "status": "ok",
     "timestamp": 1610756325623,
     "user": {
      "displayName": "김수빈",
      "photoUrl": "",
      "userId": "17754099482493494628"
     },
     "user_tz": -540
    },
    "id": "dZXr_0LWxY52",
    "outputId": "1f59102c-9394-43c3-ecb8-8679a4981ef1"
   },
   "outputs": [],
   "source": [
    "get_line_token_count = lambda sent : len(sent.split())\n",
    "get_token_count = lambda sents : reduce(lambda x, y : x + y, map(get_line_token_count, sents))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    article_dist = pd.DataFrame(columns=['Title', 'Media', 'Line length', 'Token Number'])\n",
    "    for idx, media_path in enumerate(iglob(os.path.join(ORIGIN_PATH, '**.csv'), recursive=False)):\n",
    "\n",
    "        media_name = get_media_name(media_path)\n",
    "        \n",
    "        f = open(media_path, 'r', newline=\"\\n\", encoding=\"utf-8\")\n",
    "        rdr = csv.reader(f)\n",
    "        for [title, contents] in rdr:\n",
    "            article = Article(title, media_name, contents.split(\"\\t\"))\n",
    "            if not article.content : continue # 본문이 없는 경우를 제외함\n",
    "            \n",
    "            contents = list(article.readContent())\n",
    "            dist= {'Title' : article.title, 'Media' : article.media, 'Line length' : len(contents) , 'Token Number' : get_token_count(contents)}\n",
    "            article_dist = article_dist.append(dist, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(article_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dist['Line length'] = article_dist['Line length'].astype('float64')\n",
    "article_dist['Token Number'] = article_dist['Token Number'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 수집된 기사의 문장 수 분포를 그래프로 나타낸 것이다.  \n",
    "대다수의 기사는 20개 이내의 문장 수를 가지는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "line_len_filter = article_dist['Line length'] <= 80\n",
    "\n",
    "line_min = np.min(article_dist[line_len_filter]['Line length'])\n",
    "line_max = np.max(article_dist[line_len_filter]['Line length'])\n",
    "sns.kdeplot(article_dist[line_len_filter]['Line length'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "plt.title(\"Text Line Count Distribution\")\n",
    "plt.xticks(np.arange(line_min, line_max, step=int(line_max // 15)), \\\n",
    "           [\"{}\".format(int(x)) for x in np.arange(line_min, line_max, step=int(line_max // 15))])\n",
    "plt.xlabel('Line Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20개 이하 문장 수를 가지는 기사에 한하여 토큰 개수를 그래프로 나타낸 것이다.  \n",
    "250개 이내의 토큰을 가지는 기사들이 대부분이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_len_filter = article_dist['Line length'] <= 20\n",
    "\n",
    "token_min = np.min(article_dist[line_len_filter]['Token Number'])\n",
    "token_max = np.max(article_dist[line_len_filter]['Token Number'])\n",
    "\n",
    "sns.kdeplot(article_dist[line_len_filter]['Token Number'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "plt.title(\"Text Line Token Count Distribution\")\n",
    "plt.xticks(np.arange(token_min, token_max, step=int(token_max // 20)), \\\n",
    "           [\"{}\".format(int(x)) for x in np.arange(token_min, token_max, step=int(token_max // 20))])\n",
    "plt.xlabel('Token Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 `MAX_LINE`에 따라 기사 본문을 분할하는 내용이다.  \n",
    "각 기사 본문을 40 문장 이하로 분할하여, 문장 수 분포와 토큰의 개수 분포를 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LINE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by_max_len = lambda sents : [sents[idx*MAX_LINE : idx*MAX_LINE + MAX_LINE] for idx in range(math.ceil(len(sents) / MAX_LINE))]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    article_dist = pd.DataFrame(columns=['Title', 'Media', 'Line length', 'Token Number'])\n",
    "    for idx, media_path in enumerate(iglob(os.path.join(ORIGIN_PATH, '**.csv'), recursive=False)):\n",
    "\n",
    "        media_name = get_media_name(media_path)\n",
    "        \n",
    "        f = open(media_path, 'r', newline=\"\\n\", encoding=\"utf-8\")\n",
    "        rdr = csv.reader(f)\n",
    "        for [title, contents] in rdr:\n",
    "            article = Article(title, media_name, contents.split(\"\\t\"))\n",
    "            if not article.content : continue # 본문이 없는 경우를 제외함\n",
    "            \n",
    "            contents = list(article.readContent())\n",
    "            for idx, cont in enumerate(split_by_max_len(contents)):\n",
    "                dist= {'Title' : article.title + \"--{}\".format(idx), 'Media' : article.media, \\\n",
    "                       'Line length' : len(cont) , 'Token Number' : get_token_count(cont)}\n",
    "                article_dist = article_dist.append(dist, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_min = np.min(article_dist['Line length'])\n",
    "line_max = np.max(article_dist['Line length'])\n",
    "\n",
    "print(\"Line Count : {line_min} ~ {line_max}\".format(line_max=line_max, line_min=line_min))\n",
    "sns.kdeplot(article_dist['Line length'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "plt.title(\"Text Line Count Distribution\")\n",
    "plt.xticks(np.arange(line_min, line_max, step=int(line_max // 15)), \\\n",
    "           [\"{}\".format(int(x)) for x in np.arange(line_min, line_max, step=int(line_max // 15))])\n",
    "plt.xlabel('Line Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대다수의 토큰의 개수가 300개 이내임을 확인할 수 있다.  \n",
    "이 이상의 토큰 수를 가지는 기사를 제외할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_min = np.min(article_dist['Token Number'])\n",
    "token_max = np.max(article_dist['Token Number'])\n",
    "\n",
    "line_len_filter = article_dist['Line length'] <= 40\n",
    "print(\"Token Count : {token_min} ~ {token_max}\".format(token_min=token_min, token_max=token_max))\n",
    "\n",
    "sns.kdeplot(article_dist[line_len_filter]['Token Number'])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)\n",
    "plt.title(\"Text Line Token Count Distribution\")\n",
    "plt.xticks(np.arange(token_min, token_max, step=int(token_max // 20)), \\\n",
    "           [\"{}\".format(int(x)) for x in np.arange(token_min, token_max, step=int(token_max // 20))])\n",
    "plt.xlabel('Token Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMnJhI3kxyw2hvIausnF+TX",
   "collapsed_sections": [],
   "name": "Text-preprocessing-python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
