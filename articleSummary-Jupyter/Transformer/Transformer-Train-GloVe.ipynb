{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 모델의 구성 -> Transformer_Implement.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Transformer_Implement.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Handle_Dir.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Embedding.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Encoder.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import Transformer_Implement as ti\n",
    "from CommonModule.Handle_Dir import mkdir_p, del_folder\n",
    "from CommonModule.Embedding import Embedding\n",
    "from CommonModule.Encoder import IntegerEncoder\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import iglob\n",
    "from glove import Corpus, Glove\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Setting -> GPU 1번 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/ksb/TestSampleDir\"\n",
    "DATA_BASE_DIR = os.path.join(BASE_DIR, 'articles')\n",
    "\n",
    "PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Preprocessed-Data\")\n",
    "\n",
    "SUMMARY_PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Summary-Preprocessed-Data\")\n",
    "\n",
    "WORD_EMBEDDING_DIR = os.path.join(os.path.join(BASE_DIR, 'articleSummary-Jupyter'), 'Word-Embedding-Model')\n",
    "MODEL_DIR = os.path.join(os.path.join(BASE_DIR, 'articleSummary-Jupyter'), 'Transformer-Model')\n",
    "\n",
    "glove_model_path = os.path.join(WORD_EMBEDDING_DIR, 'glove-256.model')\n",
    "input_corpus_model_path = os.path.join(WORD_EMBEDDING_DIR, 'input-corpus-256.model')\n",
    "summary_corpus_model_path = os.path.join(WORD_EMBEDDING_DIR, 'summary-corpus-256.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "말뭉치와 함께 임베딩 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove.load(glove_model_path)\n",
    "input_corpus = Corpus.load(input_corpus_model_path)\n",
    "summary_corpus = Corpus.load(summary_corpus_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "origin_embedding = Embedding(PREPROCESSED_PATH, corpus=input_corpus, glove=glove, model='GloVe')\n",
    "origin_encoded_list = origin_embedding.get_embedded_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145344"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(origin_encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_embedding = Embedding(SUMMARY_PREPROCESSED_PATH, corpus=summary_corpus, glove=glove, model='GloVe')\n",
    "summary_encoded_list = summary_embedding.get_embedded_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145145"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'model-type' : 'GloVe',\n",
    "    'inv_wv' : None,\n",
    "    'corpus' : input_corpus\n",
    "}\n",
    "output_encoded_list = IntegerEncoder(list(iglob(os.path.join(SUMMARY_PREPROCESSED_PATH, '**.csv'), recursive=False)), options).encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "LAYER_NUM = 6\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "INPUT_VOCAB_SIZE = len(input_corpus.dictionary)\n",
    "SUMMARY_VOCAB_SIZE = len(summary_corpus.dictionary)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "WARMUP_STEPS = 50\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_length = lambda x : np.max([x[idx].shape[0] for idx in range(len(x))]) \n",
    "MAX_LEN = get_max_length(origin_encoded_list)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41594"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_idx_max_length = lambda x : np.argmax([len(x[idx]) for idx in range(len(x))]) \n",
    "idx = get_idx_max_length(origin_encoded_list)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(split_num, length):\n",
    "    split_size = length // split_num\n",
    "    \n",
    "    split_range = []\n",
    "    for num in range(split_num):\n",
    "        if num == split_num -1:\n",
    "            split_range.append(np.arange(num * split_size, length))\n",
    "        else:\n",
    "            split_range.append(np.arange(num * split_size, (num + 1) * split_size))\n",
    "    return split_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "origin_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    origin_encoded_list, maxlen=MAX_LEN, padding='post')\n",
    "summary_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    summary_encoded_list, maxlen=MAX_LEN, padding='post')\n",
    "output_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    output_encoded_list, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents Shape : (4853, 349, 256)\n",
      "Summaries Shape : (4853, 349, 256)\n",
      "Output Shape : (4853, 349)\n"
     ]
    }
   ],
   "source": [
    "print('Contents Shape : {}'.format(origin_encoded_matrix.shape))\n",
    "print('Summaries Shape : {}'.format(summary_encoded_matrix.shape))\n",
    "print('Output Shape : {}'.format(output_encoded_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'encoder_inputs': origin_encoded_matrix, # Encoder Input\n",
    "        'decoder_inputs': summary_encoded_matrix[:, :-1, :] # Decoder Input\n",
    "    },\n",
    "    {\n",
    "        # Decoder Output, Remove <SOS>\n",
    "        'Output': output_encoded_matrix[:, 1:]  \n",
    "    },\n",
    "))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (64, 349, 256), decoder input : (64, 348, 256)\n",
      "output shape : (64, 348)\n",
      "encoder input : (42, 349, 256), decoder input : (42, 348, 256)\n",
      "output shape : (42, 348)\n"
     ]
    }
   ],
   "source": [
    "for dict_1, dict_2 in dataset.as_numpy_iterator():\n",
    "    print(\"encoder input : {enc}, decoder input : {dec}\".format(enc= dict_1['encoder_inputs'].shape,\n",
    "                                                               dec=dict_1['decoder_inputs'].shape))\n",
    "    print(\"output shape : {}\".format(dict_2['Output'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률 Learning Rate 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate_scheduler = ti.CustomSchedule(d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = 0.9  \n",
    "beta_2 = 0.98\n",
    "epsilon = 10 ** -9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lrate_scheduler, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 63677, 256)\n",
      "encoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_0 sub-layer 2\n",
      "encoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_1 sub-layer 2\n",
      "encoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_2 sub-layer 2\n",
      "encoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_3 sub-layer 2\n",
      "encoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_4 sub-layer 2\n",
      "encoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_5 sub-layer 2\n",
      "(1, 63677, 256)\n",
      "decoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 3\n",
      "decoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 3\n",
      "decoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 3\n",
      "decoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 3\n",
      "decoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 3\n",
      "decoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 3\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model = ti.Transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        layer_num=LAYER_NUM,\n",
    "        dff=DFF,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout = 0.3).get_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "\n",
    "    y_pred_rank = y_pred.shape.ndims\n",
    "    y_true_rank = y_true.shape.ndims\n",
    "    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
    "        y_true = array_ops.squeeze(y_true, [-1])\n",
    "    y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if K.dtype(y_pred) != K.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
    "    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    print(\"loss func\", y_true.shape, y_pred.shape)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (Functional)            (None, None, 256)    3162624     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (Functional)            (None, None, 256)    4744704     decoder_inputs[0][0]             \n",
      "                                                                 Encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, None, 63677)  16364989    Decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,272,317\n",
      "Trainable params: 24,272,317\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['loss'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "optimizer Tensor(\"Adam/Cast:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "optimizer return Tensor(\"Adam/mul_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "optimizer Tensor(\"Adam/Cast:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "optimizer return Tensor(\"Adam/mul_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "loss func (None, 348) (None, 348, 63677)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "accuracy  (None, 348) (None, 348, 63677)\n",
      "61/61 - 134s - loss: 0.7052 - accuracy: 5.8478e-05 - val_loss: 0.6999 - val_accuracy: 2.6870e-04\n",
      "Epoch 2/70\n",
      "61/61 - 113s - loss: 0.6935 - accuracy: 4.4044e-04 - val_loss: 0.6799 - val_accuracy: 6.8175e-04\n",
      "Epoch 3/70\n",
      "61/61 - 113s - loss: 0.6694 - accuracy: 5.1076e-04 - val_loss: 0.6507 - val_accuracy: 6.8175e-04\n",
      "Epoch 4/70\n",
      "61/61 - 112s - loss: 0.6395 - accuracy: 5.1372e-04 - val_loss: 0.6203 - val_accuracy: 6.8175e-04\n",
      "Epoch 5/70\n",
      "61/61 - 113s - loss: 0.6109 - accuracy: 6.5214e-04 - val_loss: 0.5942 - val_accuracy: 6.8175e-04\n",
      "Epoch 6/70\n",
      "61/61 - 112s - loss: 0.5885 - accuracy: 6.8175e-04 - val_loss: 0.5771 - val_accuracy: 6.8175e-04\n",
      "Epoch 7/70\n",
      "61/61 - 112s - loss: 0.5763 - accuracy: 6.8175e-04 - val_loss: 0.5710 - val_accuracy: 6.8175e-04\n",
      "Epoch 8/70\n",
      "61/61 - 112s - loss: 0.5726 - accuracy: 6.8175e-04 - val_loss: 0.5696 - val_accuracy: 6.8175e-04\n",
      "Epoch 9/70\n",
      "61/61 - 112s - loss: 0.5716 - accuracy: 6.8175e-04 - val_loss: 0.5691 - val_accuracy: 6.8175e-04\n",
      "Epoch 10/70\n",
      "61/61 - 112s - loss: 0.5711 - accuracy: 6.8175e-04 - val_loss: 0.5690 - val_accuracy: 6.8175e-04\n",
      "Epoch 11/70\n",
      "61/61 - 113s - loss: 0.5702 - accuracy: 6.8175e-04 - val_loss: 0.5690 - val_accuracy: 6.8175e-04\n",
      "Epoch 12/70\n",
      "61/61 - 113s - loss: 0.5679 - accuracy: 6.8175e-04 - val_loss: 0.5693 - val_accuracy: 6.8175e-04\n",
      "Epoch 13/70\n",
      "61/61 - 113s - loss: 0.5647 - accuracy: 6.8175e-04 - val_loss: 0.5703 - val_accuracy: 6.8175e-04\n",
      "Epoch 14/70\n",
      "61/61 - 113s - loss: 0.5612 - accuracy: 6.8175e-04 - val_loss: 0.5723 - val_accuracy: 6.8175e-04\n",
      "Epoch 15/70\n",
      "61/61 - 113s - loss: 0.5580 - accuracy: 6.8175e-04 - val_loss: 0.5751 - val_accuracy: 6.8175e-04\n",
      "Epoch 16/70\n",
      "61/61 - 114s - loss: 0.5560 - accuracy: 6.8175e-04 - val_loss: 0.5774 - val_accuracy: 6.8175e-04\n",
      "Epoch 17/70\n",
      "61/61 - 114s - loss: 0.5538 - accuracy: 6.8175e-04 - val_loss: 0.5805 - val_accuracy: 6.8175e-04\n",
      "Epoch 18/70\n",
      "61/61 - 114s - loss: 0.5525 - accuracy: 6.8175e-04 - val_loss: 0.5819 - val_accuracy: 6.8175e-04\n",
      "Epoch 19/70\n",
      "61/61 - 114s - loss: 0.5524 - accuracy: 6.8101e-04 - val_loss: 0.5843 - val_accuracy: 6.8175e-04\n",
      "Epoch 20/70\n",
      "61/61 - 114s - loss: 0.5549 - accuracy: 6.8175e-04 - val_loss: 0.5845 - val_accuracy: 6.8175e-04\n",
      "Epoch 21/70\n",
      "61/61 - 115s - loss: 0.5550 - accuracy: 6.8175e-04 - val_loss: 0.5902 - val_accuracy: 6.8175e-04\n",
      "Epoch 22/70\n",
      "61/61 - 115s - loss: 0.5589 - accuracy: 6.6769e-04 - val_loss: 0.5938 - val_accuracy: 6.8175e-04\n",
      "Epoch 23/70\n",
      "61/61 - 114s - loss: 0.5830 - accuracy: 6.6324e-04 - val_loss: 0.5939 - val_accuracy: 6.8175e-04\n",
      "Epoch 24/70\n",
      "61/61 - 114s - loss: 0.5784 - accuracy: 6.8101e-04 - val_loss: 0.5861 - val_accuracy: 6.8175e-04\n",
      "Epoch 25/70\n",
      "61/61 - 114s - loss: 0.5710 - accuracy: 6.7879e-04 - val_loss: 0.5817 - val_accuracy: 6.8175e-04\n",
      "Epoch 26/70\n",
      "61/61 - 114s - loss: 0.5634 - accuracy: 6.8175e-04 - val_loss: 0.5871 - val_accuracy: 6.8175e-04\n",
      "Epoch 27/70\n",
      "61/61 - 114s - loss: 0.5627 - accuracy: 6.7731e-04 - val_loss: 0.5845 - val_accuracy: 6.8175e-04\n",
      "Epoch 28/70\n",
      "61/61 - 115s - loss: 0.5646 - accuracy: 6.8175e-04 - val_loss: 0.5859 - val_accuracy: 6.8175e-04\n",
      "Epoch 29/70\n",
      "61/61 - 115s - loss: 0.5743 - accuracy: 6.8249e-04 - val_loss: 0.5921 - val_accuracy: 6.8175e-04\n",
      "Epoch 30/70\n",
      "61/61 - 116s - loss: 0.5852 - accuracy: 6.8175e-04 - val_loss: 0.5781 - val_accuracy: 6.8175e-04\n",
      "Epoch 31/70\n",
      "61/61 - 116s - loss: 0.5775 - accuracy: 6.8175e-04 - val_loss: 0.5754 - val_accuracy: 6.8175e-04\n",
      "Epoch 32/70\n",
      "61/61 - 115s - loss: 0.5757 - accuracy: 6.8175e-04 - val_loss: 0.5749 - val_accuracy: 6.8175e-04\n",
      "Epoch 33/70\n",
      "61/61 - 116s - loss: 0.5749 - accuracy: 6.8175e-04 - val_loss: 0.5747 - val_accuracy: 6.8175e-04\n",
      "Epoch 34/70\n",
      "61/61 - 115s - loss: 0.5742 - accuracy: 6.8175e-04 - val_loss: 0.5746 - val_accuracy: 6.8175e-04\n",
      "Epoch 35/70\n",
      "61/61 - 115s - loss: 0.5736 - accuracy: 6.8175e-04 - val_loss: 0.5745 - val_accuracy: 6.8175e-04\n",
      "Epoch 36/70\n",
      "61/61 - 116s - loss: 0.5734 - accuracy: 6.8175e-04 - val_loss: 0.5743 - val_accuracy: 6.8175e-04\n",
      "Epoch 37/70\n",
      "61/61 - 116s - loss: 0.5732 - accuracy: 6.8175e-04 - val_loss: 0.5741 - val_accuracy: 6.8175e-04\n",
      "Epoch 38/70\n",
      "61/61 - 116s - loss: 0.5730 - accuracy: 6.8175e-04 - val_loss: 0.5728 - val_accuracy: 6.8175e-04\n",
      "Epoch 39/70\n",
      "61/61 - 115s - loss: 0.5738 - accuracy: 6.7287e-04 - val_loss: 0.5753 - val_accuracy: 6.8175e-04\n",
      "Epoch 40/70\n",
      "61/61 - 115s - loss: 0.5747 - accuracy: 6.7583e-04 - val_loss: 0.5722 - val_accuracy: 6.8175e-04\n",
      "Epoch 41/70\n",
      "61/61 - 116s - loss: 0.5734 - accuracy: 6.8175e-04 - val_loss: 0.5710 - val_accuracy: 6.8175e-04\n",
      "Epoch 42/70\n",
      "61/61 - 116s - loss: 0.5726 - accuracy: 6.8175e-04 - val_loss: 0.5702 - val_accuracy: 6.8175e-04\n",
      "Epoch 43/70\n",
      "61/61 - 115s - loss: 0.5722 - accuracy: 6.8175e-04 - val_loss: 0.5695 - val_accuracy: 6.8175e-04\n",
      "Epoch 44/70\n",
      "61/61 - 115s - loss: 0.5722 - accuracy: 6.8175e-04 - val_loss: 0.5692 - val_accuracy: 6.8175e-04\n",
      "Epoch 45/70\n",
      "61/61 - 115s - loss: 0.5724 - accuracy: 6.8175e-04 - val_loss: 0.5691 - val_accuracy: 6.8175e-04\n",
      "Epoch 46/70\n",
      "61/61 - 115s - loss: 0.5725 - accuracy: 6.8175e-04 - val_loss: 0.5690 - val_accuracy: 6.8175e-04\n",
      "Epoch 47/70\n",
      "61/61 - 115s - loss: 0.5725 - accuracy: 6.8175e-04 - val_loss: 0.5690 - val_accuracy: 6.8175e-04\n",
      "Epoch 48/70\n",
      "61/61 - 116s - loss: 0.5723 - accuracy: 6.6843e-04 - val_loss: 0.5693 - val_accuracy: 6.8175e-04\n",
      "Epoch 49/70\n",
      "61/61 - 115s - loss: 0.5711 - accuracy: 6.6250e-04 - val_loss: 0.5728 - val_accuracy: 6.8175e-04\n",
      "Epoch 50/70\n",
      "61/61 - 116s - loss: 0.5674 - accuracy: 6.7879e-04 - val_loss: 0.5791 - val_accuracy: 6.8175e-04\n",
      "Epoch 51/70\n",
      "61/61 - 115s - loss: 0.5735 - accuracy: 6.8323e-04 - val_loss: 0.5767 - val_accuracy: 6.8175e-04\n",
      "Epoch 52/70\n",
      "61/61 - 115s - loss: 0.5774 - accuracy: 6.8175e-04 - val_loss: 0.5732 - val_accuracy: 6.8175e-04\n",
      "Epoch 53/70\n",
      "61/61 - 115s - loss: 0.5739 - accuracy: 6.8175e-04 - val_loss: 0.5744 - val_accuracy: 6.8175e-04\n",
      "Epoch 54/70\n",
      "61/61 - 115s - loss: 0.5732 - accuracy: 6.8175e-04 - val_loss: 0.5763 - val_accuracy: 6.8175e-04\n",
      "Epoch 55/70\n",
      "61/61 - 117s - loss: 0.5729 - accuracy: 6.8175e-04 - val_loss: 0.5780 - val_accuracy: 6.8175e-04\n",
      "Epoch 56/70\n",
      "61/61 - 115s - loss: 0.5734 - accuracy: 6.8175e-04 - val_loss: 0.5787 - val_accuracy: 6.8175e-04\n",
      "Epoch 57/70\n",
      "61/61 - 115s - loss: 0.5733 - accuracy: 6.8175e-04 - val_loss: 0.5776 - val_accuracy: 6.8175e-04\n",
      "Epoch 58/70\n",
      "61/61 - 115s - loss: 0.5728 - accuracy: 6.8175e-04 - val_loss: 0.5773 - val_accuracy: 6.8175e-04\n",
      "Epoch 59/70\n",
      "61/61 - 116s - loss: 0.5727 - accuracy: 6.8175e-04 - val_loss: 0.5759 - val_accuracy: 6.8175e-04\n",
      "Epoch 60/70\n",
      "61/61 - 117s - loss: 0.5729 - accuracy: 6.8175e-04 - val_loss: 0.5738 - val_accuracy: 6.8175e-04\n",
      "Epoch 61/70\n",
      "61/61 - 115s - loss: 0.5729 - accuracy: 6.8249e-04 - val_loss: 0.5735 - val_accuracy: 6.8175e-04\n",
      "Epoch 62/70\n",
      "61/61 - 115s - loss: 0.5724 - accuracy: 6.8175e-04 - val_loss: 0.5731 - val_accuracy: 6.8175e-04\n",
      "Epoch 63/70\n",
      "61/61 - 114s - loss: 0.5720 - accuracy: 6.8175e-04 - val_loss: 0.5732 - val_accuracy: 6.8175e-04\n",
      "Epoch 64/70\n",
      "61/61 - 114s - loss: 0.5718 - accuracy: 6.8175e-04 - val_loss: 0.5734 - val_accuracy: 6.8175e-04\n",
      "Epoch 65/70\n",
      "61/61 - 114s - loss: 0.5717 - accuracy: 6.8175e-04 - val_loss: 0.5736 - val_accuracy: 6.8175e-04\n",
      "Epoch 66/70\n",
      "61/61 - 115s - loss: 0.5717 - accuracy: 6.8175e-04 - val_loss: 0.5740 - val_accuracy: 6.8175e-04\n",
      "Epoch 67/70\n",
      "61/61 - 114s - loss: 0.5715 - accuracy: 6.8175e-04 - val_loss: 0.5743 - val_accuracy: 6.8175e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/70\n",
      "61/61 - 114s - loss: 0.5713 - accuracy: 6.8175e-04 - val_loss: 0.5748 - val_accuracy: 6.8175e-04\n",
      "Epoch 69/70\n",
      "61/61 - 114s - loss: 0.5711 - accuracy: 6.8175e-04 - val_loss: 0.5741 - val_accuracy: 6.8175e-04\n",
      "Epoch 70/70\n",
      "61/61 - 115s - loss: 0.5718 - accuracy: 6.8175e-04 - val_loss: 0.5747 - val_accuracy: 6.8175e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='accuracy', factor=0.5, patience=10, min_lr=0.00005, verbose=1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(dataset, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, shuffle=True, validation_data=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p(MODEL_DIR)\n",
    "model_num = len(list(iglob(os.path.join(MODEL_DIR, '**.h5'), recursive=False)))\n",
    "model_path = os.path.join(MODEL_DIR, 'transformer-{}.h5'.format(model_num))\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
