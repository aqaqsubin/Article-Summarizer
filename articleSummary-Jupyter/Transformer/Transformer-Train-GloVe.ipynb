{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 모델의 구성 -> Transformer_Implement.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Transformer_Implement.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Handle_Dir.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Embedding.ipynb\n",
      "importing Jupyter notebook from /data/ksb/TestSampleDir/articleSummary-Jupyter/Transformer/CommonModule/Encoder.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import Transformer_Implement as ti\n",
    "from CommonModule.Handle_Dir import mkdir_p, del_folder\n",
    "from CommonModule.Embedding import Embedding\n",
    "from CommonModule.Encoder import IntegerEncoder\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import iglob\n",
    "from glove import Corpus, Glove\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Setting -> GPU 1번 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/ksb/TestSampleDir\"\n",
    "DATA_BASE_DIR = os.path.join(BASE_DIR, 'articles')\n",
    "\n",
    "PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Preprocessed-Data\")\n",
    "\n",
    "SUMMARY_PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Summary-Preprocessed-Data\")\n",
    "\n",
    "MODEL_BASE_DIR = os.path.join(os.path.join(BASE_DIR, 'articleSummary-Jupyter'), 'Word-Embedding-Model')\n",
    "glove_model_path = os.path.join(MODEL_BASE_DIR, 'glove-256.model')\n",
    "corpus_model_path = os.path.join(MODEL_BASE_DIR, 'corpus-256.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "말뭉치와 함께 임베딩 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = Glove.load(glove_model_path)\n",
    "corpus = Corpus.load(corpus_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_embedding = Embedding(PREPROCESSED_PATH, corpus=corpus, glove=glove, model='GloVe')\n",
    "train_encoded_list = tr_embedding.get_embedded_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예외 상황 - 중앙일보만 테스트\n",
    "\n",
    "import csv\n",
    "\n",
    "train_encoded_list = []\n",
    "\n",
    "test_media_list = [os.path.join(PREPROCESSED_PATH, '중앙일보.csv')]\n",
    "for _, proc_path in enumerate(test_media_list):\n",
    "    f = open(proc_path, 'r', newline=\"\\n\", encoding=\"utf-8\")\n",
    "            \n",
    "    for [article_idx, title, contents] in csv.reader(f):  \n",
    "        content = contents.split(\"\\t\")\n",
    "                \n",
    "        vec = [glove.word_vectors[corpus.dictionary[token]] \n",
    "                for sent in content for token in sent.split() if token in corpus.dictionary]\n",
    "        train_encoded_list.append(np.array(vec))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_embedding = Embedding(SUMMARY_PREPROCESSED_PATH, corpus=corpus, glove=glove, model='GloVe')\n",
    "test_encoded_list = te_embedding.get_embedded_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'model-type' : 'GloVe',\n",
    "    'inv_wv' : None,\n",
    "    'corpus' : corpus\n",
    "}\n",
    "output_encoded_list = IntegerEncoder(list(iglob(os.path.join(SUMMARY_PREPROCESSED_PATH, '**.csv'), recursive=False)), options).encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "LAYER_NUM = 6\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "VOCAB_SIZE = len(corpus.dictionary)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "WARMUP_STEPS = 50\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_length = lambda x : np.max([len(x[idx]) for idx in range(len(x))])\n",
    "MAX_LEN = get_max_length(train_encoded_list)\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기사의 시작과 끝을 알리는 <*SOS*>,<*EOS*>을 붙이고,  \n",
    "nd-array로 저장하기 위해 패딩을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "traing_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    train_encoded_list, maxlen=MAX_LEN, padding='post')\n",
    "test_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    test_encoded_list, maxlen=MAX_LEN, padding='post')\n",
    "output_encoded_matrix = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    output_encoded_list, maxlen=MAX_LEN, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents Shape : (4853, 374, 256)\n",
      "Summaries Shape : (4853, 374, 256)\n",
      "Output Shape : (4853, 374)\n"
     ]
    }
   ],
   "source": [
    "print('Contents Shape : {}'.format(traing_encoded_matrix.shape))\n",
    "print('Summaries Shape : {}'.format(test_encoded_matrix.shape))\n",
    "print('Output Shape : {}'.format(output_encoded_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정수 인코딩을 수행한 결과행렬에 정수를 임베딩 벡터로 변환한다.  \n",
    "이의 결과로 (59604, 4392, 256)의 dimension을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'encoder_inputs': traing_encoded_matrix, # Encoder Input\n",
    "        'decoder_inputs': test_encoded_matrix[:, :-1, :] # Decoder Input\n",
    "    },\n",
    "    {\n",
    "        # Decoder Output, Remove <SOS>\n",
    "        'Output': output_encoded_matrix[:, 1:]  \n",
    "    },\n",
    "))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (64, 374, 256), decoder input : (64, 373, 256)\n",
      "output shape : (64, 373)\n",
      "encoder input : (53, 374, 256), decoder input : (53, 373, 256)\n",
      "output shape : (53, 373)\n"
     ]
    }
   ],
   "source": [
    "for dict_1, dict_2 in dataset.as_numpy_iterator():\n",
    "    print(\"encoder input : {enc}, decoder input : {dec}\".format(enc= dict_1['encoder_inputs'].shape,\n",
    "                                                               dec=dict_1['decoder_inputs'].shape))\n",
    "    print(\"output shape : {}\".format(dict_2['Output'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률 Learning Rate 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate_scheduler = ti.CustomSchedule(d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1 = 0.9  \n",
    "beta_2 = 0.98\n",
    "epsilon = 10 ** -9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lrate_scheduler, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 174477, 256)\n",
      "encoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_0 sub-layer 2\n",
      "encoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_1 sub-layer 2\n",
      "encoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_2 sub-layer 2\n",
      "encoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_3 sub-layer 2\n",
      "encoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_4 sub-layer 2\n",
      "encoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_5 sub-layer 2\n",
      "(1, 174477, 256)\n",
      "decoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 3\n",
      "decoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 3\n",
      "decoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 3\n",
      "decoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 3\n",
      "decoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 3\n",
      "decoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 3\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model = ti.Transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        layer_num=LAYER_NUM,\n",
    "        dff=DFF,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS).get_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def sparse_categorical_accuracy(y_true, y_pred):\n",
    "    y_pred = ops.convert_to_tensor_v2_with_dispatch(y_pred)\n",
    "    y_true = ops.convert_to_tensor_v2_with_dispatch(y_true)\n",
    "\n",
    "    y_pred_rank = y_pred.shape.ndims\n",
    "    y_true_rank = y_true.shape.ndims\n",
    "    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
    "        y_true = array_ops.squeeze(y_true, [-1])\n",
    "    y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if K.dtype(y_pred) != K.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
    "    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (Functional)            (None, None, 256)    3162624     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (Functional)            (None, None, 256)    4744704     decoder_inputs[0][0]             \n",
      "                                                                 Encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, None, 174477) 44840589    Decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 52,747,917\n",
      "Trainable params: 52,747,917\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    print(\"loss func\", y_true.shape, y_pred.shape)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    print(\"loss func\", y_true.shape, y_pred.shape)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "    print(\"accuracy \", y_true.shape, y_pred.shape)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n",
    "    print(\"accuracy \", y_true.shape, y_pred.shape)\n",
    "    return sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "loss func (None, 373) (None, 373, 174477)\n",
      "loss func (None, 373) (None, 373, 174477)\n",
      "optimizer Tensor(\"Adam/Cast:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "optimizer return Tensor(\"Adam/mul_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "accuracy  (None, 373) (None, 373, 174477)\n",
      "accuracy  (None, 373) (None, 373, 174477)\n",
      "loss func (None, 373) (None, 373, 174477)\n",
      "loss func (None, 373) (None, 373, 174477)\n",
      "optimizer Tensor(\"Adam/Cast:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "optimizer return Tensor(\"Adam/mul_1:0\", shape=(), dtype=float32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "accuracy  (None, 373) (None, 373, 174477)\n",
      "accuracy  (None, 373) (None, 373, 174477)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,8,374,374] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Transformer/Encoder/encoder_layer_0/tf.nn.softmax/Softmax (defined at <ipython-input-19-a392481a996e>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/Transformer/Decoder/decoder_layer_3/dense_71/Tensordot/MatMul/MatMul_1/_1478]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,8,374,374] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Transformer/Encoder/encoder_layer_0/tf.nn.softmax/Softmax (defined at <ipython-input-19-a392481a996e>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_40633]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a392481a996e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreduce_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,8,374,374] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Transformer/Encoder/encoder_layer_0/tf.nn.softmax/Softmax (defined at <ipython-input-19-a392481a996e>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/Transformer/Decoder/decoder_layer_3/dense_71/Tensordot/MatMul/MatMul_1/_1478]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,8,374,374] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node Transformer/Encoder/encoder_layer_0/tf.nn.softmax/Softmax (defined at <ipython-input-19-a392481a996e>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_40633]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.00005, verbose=1)\n",
    "\n",
    "model.fit(dataset, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, shuffle=True, callbacks=[reduce_lr])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
