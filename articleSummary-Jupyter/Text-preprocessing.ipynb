{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\aqaq9\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\aqaq9\\anaconda3\\lib\\site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aqaq9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from shutil import rmtree\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \".\"\n",
    "ARTICLE_MEDIA_PATH = os.path.join(BASE_DIR,\"articles\")\n",
    "TARGET_PATH = os.path.join(BASE_DIR,\"preprocessed\")\n",
    "SWORDS_FILE_PATH = os.path.join(BASE_DIR, \"StopWordList.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    import errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def del_folder(path):\n",
    "    try:\n",
    "        rmtree(path)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArticle(filename):\n",
    "\n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    title = f.readline()[:-1]\n",
    "    content = f.readline()[:-1]\n",
    "    media = f.readline()[:-1]\n",
    "    f.close()\n",
    "\n",
    "    return title, media, content\n",
    "\n",
    "def cleanContent(content, media):\n",
    "    content = re.sub('\\s+', ' ', content)  # 중복 공백, 탭, 개행 제거\n",
    "    content = re.sub(r'\\([^)]*\\)', '', content)  # 괄호 안 숫자 제거\n",
    "    content = content.replace(media, '')  # 언론사명 제거\n",
    "\n",
    "    return content\n",
    "\n",
    "def removeSpecialChar(text):\n",
    "    retokenize = RegexpTokenizer(\"[\\w]+\")\n",
    "    return ' '.join(retokenize.tokenize(text))\n",
    "\n",
    "def getStopWord(swords_filename):\n",
    "    swords = []\n",
    "    with open(swords_filename, 'r') as f:\n",
    "        swords = f.readlines()\n",
    "        swords = [sword.strip() for sword in swords]\n",
    "\n",
    "    return swords\n",
    "\n",
    "def delStopWord(sentence):\n",
    "    if sentence is '':\n",
    "        return None\n",
    "\n",
    "    okt = Okt()\n",
    "    swords = getStopWord(SWORDS_FILE_PATH)\n",
    "    return ' '.join([word for word in okt.morphs(sentence) if word not in swords])\n",
    "\n",
    "def getRmSwordSentences(sentences):\n",
    "    rmSwordSentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = delStopWord(sentence)\n",
    "        if sentence is not None : rmSwordSentences.append(sentence)\n",
    "\n",
    "    return rmSwordSentences\n",
    "\n",
    "def savePreprocessedText(media, article, nouns):\n",
    "\n",
    "    mkdir_p(os.path.join(TARGET_PATH, media))\n",
    "    save_path = os.path.join(os.path.join(TARGET_PATH, media), article)\n",
    "\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(title)\n",
    "        preprocessed = \"\"\n",
    "        for noun in nouns:\n",
    "            preprocessed += noun + \"/\"\n",
    "        f.write(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    media_list = os.listdir(ARTICLE_MEDIA_PATH)\n",
    "\n",
    "    for media in media_list:\n",
    "\n",
    "        media_path = os.path.join(ARTICLE_MEDIA_PATH, media)\n",
    "        article_list = os.listdir(media_path)\n",
    "\n",
    "        for article in article_list:\n",
    "            title, media, content = readArticle(os.path.join(media_path, article))\n",
    "            content = cleanContent(content, media)\n",
    "\n",
    "            sentences = sent_tokenize(content)\n",
    "            sentences = [removeSpecialChar(sentence) for sentence in sentences]\n",
    "\n",
    "            savePreprocessedText(media, article, sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
