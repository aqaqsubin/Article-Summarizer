{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, N, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.get_positional_encoding(N, d_model)\n",
    "    \n",
    "    def get_positional_encoding(self, N, d_model):\n",
    "        \n",
    "        def get_pos_matrix(pos, i, d_model):\n",
    "            pos_matrix = pos / tf.math.pow(10000, i / tf.cast(d_model, tf.float32))\n",
    "            pos_matrix = pos_matrix.numpy()\n",
    "\n",
    "            pos_matrix[:,0::2] = tf.math.sin(pos_matrix[:,0::2])\n",
    "            pos_matrix[:,1::2] = tf.math.cos(pos_matrix[:,1::2])\n",
    "\n",
    "            return pos_matrix\n",
    "\n",
    "        pos_encoding = get_pos_matrix(pos=tf.range(N, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis,:], d_model=d_model)\n",
    "   \n",
    "        print(\"Positional Encoding\", pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding[:,:], tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(multiple_qk):\n",
    "    mask = tf.cast(tf.math.equal(multiple_qk,0), tf.float32)\n",
    "    return mask * -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(attention_score_matrix):\n",
    "    N = tf.shape(attention_score_matrix)[1]\n",
    "\n",
    "    mask = tf.ones(shape=(N, N), dtype=tf.float32)\n",
    "    #mask = tf.experimental.numpy.triu(mask, 1) \n",
    "    \n",
    "    mask = 1 - tf.linalg.band_part(mask, -1, 0)\n",
    "    mask = mask[tf.newaxis, :, :] * -1e9\n",
    "\n",
    "    pad_mask = create_padding_mask(attention_score_matrix)\n",
    "    return tf.minimum(mask, pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask_type):\n",
    "\n",
    "    # Attention Score : Q * K^T\n",
    "    attention_score_matrix = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # Scaling : Divide by sqrt(d_k)\n",
    "    d_k = tf.cast(key.shape[-1], tf.float32)\n",
    "    scaled_matrix = attention_score_matrix / tf.math.sqrt(d_k)\n",
    "\n",
    "    # Padding Mask or Look-Ahead Mask\n",
    "    if mask_type is 'padding':\n",
    "        scaled_matrix += create_padding_mask(scaled_matrix)\n",
    "    elif mask_type is 'look_ahead':\n",
    "        scaled_matrix += create_look_ahead_mask(scaled_matrix)\n",
    "\n",
    "    # Softmax fuction\n",
    "    attention_weights = tf.nn.softmax(scaled_matrix, axis=-1) \n",
    "\n",
    "    # Weighted Sum : multiply V matrix\n",
    "    attention_value = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_value, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printShape(Q, K, V, status):\n",
    "    print(\"[{status}] Q shape : {q}, K shape : {k}, V shape : {v}\\n\".format(status=status, q=Q.shape, k=K.shape, v=V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"depth가 정수 형식이 아닙니다.\"\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        # Dense 층의 출력 차원은 d_model\n",
    "        self.WQ = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WK = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WV = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WO = tf.keras.layers.Dense(units=self.d_model)\n",
    "\n",
    "    \n",
    "    def get_attention(self, query, key, value, mask_type=None):\n",
    "\n",
    "        printShape(query, key, value, \"Input\")\n",
    "        \n",
    "        def split_sequences(batch_size, num_heads, d_model, query, key, value):\n",
    "            Q_list = tf.reshape(query, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "            K_list = tf.reshape(key, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "            V_list = tf.reshape(value, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "\n",
    "            return tf.transpose(Q_list, perm=[0, 2, 1, 3]), tf.transpose(K_list, perm=[0, 2, 1, 3]), tf.transpose(V_list, perm=[0, 2, 1, 3])\n",
    "            \n",
    "        # 현재 batch_size는 1이다.\n",
    "        # 모델 훈련에서의 batch 당 token의 수를 의미한다.\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q*W^Q : Dense 층 구성\n",
    "        q_WQ = self.WQ(query)\n",
    "        k_WK = self.WK(key)\n",
    "        v_WV = self.WV(value)\n",
    "        printShape(q_WQ, k_WK, v_WV, \"Dense\")\n",
    "\n",
    "        # num_heads로 입력 행렬 분할\n",
    "        # (batch_size, 입력 시퀀스 개수, d_model) -> (batch_size, num_heads, 입력 시퀀스 개수, d_model/num_heads)\n",
    "        Q_list, K_list, V_list = split_sequences(batch_size, self.num_heads, self.d_model, q_WQ, k_WK, v_WV)\n",
    "        printShape(Q_list, K_list, V_list, \"Splited\")\n",
    "\n",
    "        # Attention value \n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q_list, K_list, V_list, mask_type)\n",
    "        \n",
    "        # head를 연결하기 위한 Tensor shape 조정\n",
    "        # (batch_size, num_heads, 입력 시퀀스 개수, d_model/num_heads) -> (batch_size, 입력 시퀀스 개수, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # head 연결\n",
    "        # (batch_size, 입력 시퀀스 개수, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))\n",
    "\n",
    "        # Multi-Head 최종 결과 값\n",
    "        result = self.WO(concat_attention)\n",
    "\n",
    "        return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, N, layer_num, dff, d_model, num_heads, dropout=None):\n",
    "        self.N = N\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def stack_encode_layer(self, layer_name):\n",
    "        \n",
    "        # Input 1개  : 인코더 입력\n",
    "        inputs = tf.keras.Input(shape=(None, self.d_model), name=\"encode_inputs\")\n",
    "\n",
    "        print(layer_name, \"sub-layer 1\")\n",
    "        # encoder의 self attention은 query, key, value가 모두 입력 문장의 단어 벡터를 의미한다.\n",
    "        # query = key = value\n",
    "        query = key = value = inputs\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        multi_head_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = multi_head_attention.get_attention(query, key, value, mask_type='padding')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += inputs\n",
    "        # Normalization\n",
    "        sublayer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        print(layer_name, \"sub-layer 2\")\n",
    "        # Feed Forward Network\n",
    "        # 입력과 출력의 크기가 보존되며, FFN의 은닉층 크기는 dff다.\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.dff, activation='relu')(sublayer_output)\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.d_model)(feed_forward_net)\n",
    "        \n",
    "        feed_forward_net = tf.keras.layers.Dropout(rate=0.1)(feed_forward_net)\n",
    "        # Residual connection\n",
    "        feed_forward_net += sublayer_output\n",
    "        # Normalization\n",
    "        encoder_layer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(feed_forward_net)\n",
    "\n",
    "        return tf.keras.Model(inputs=[inputs], outputs=encoder_layer_output, name=layer_name)\n",
    "\n",
    "    def get_encoder(self):\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_inputs\")\n",
    "\n",
    "        # Positional Encoding\n",
    "        encoder_input = PositionalEncoding(self.N, self.d_model)(inputs)\n",
    "        \n",
    "        # Encoder Layer 쌓기\n",
    "        # layer_num 만큼 encoder layer를 쌓는다\n",
    "        for idx in range(self.layer_num):\n",
    "            encoder_input = encoder_output = self.stack_encode_layer(layer_name=\"encoder_layer_{}\".format(idx))(inputs=[encoder_input])\n",
    "\n",
    "        return tf.keras.Model(inputs=[inputs], outputs=encoder_output, name=\"Encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, N, layer_num, dff, d_model, num_heads, dropout=None):\n",
    "        self.N = N\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def stack_decode_layer(self, layer_name):\n",
    "        print(layer_name, \"sub-layer 1\")\n",
    "        \n",
    "        #Input 2개 : 디코더 입력, 인코더 출력\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_layer_input\")\n",
    "        encoder_output = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_output\")\n",
    "\n",
    "        # Masked Multi-Head Self Attention\n",
    "        # 디코더의 Self Attention에서 query, key, value의 출처는 디코더 입력이다.\n",
    "        query = key = value = decoder_input\n",
    "\n",
    "        self_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = self_attention.get_attention(query, key, value, mask_type='look_ahead')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += decoder_input\n",
    "        # Normalization\n",
    "        sublayer_output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        # Multi-Head Encoder-Decoder Attention\n",
    "        # 디코더 Encoder-Decoder Attention의 입력 중 Q는 디코더 sub-layer의 출력이고, K,V는 인코더의 출력이다.\n",
    "        key_from_encoder = value_from_encoder = encoder_output\n",
    "        query_from_decoder = sublayer_output_1\n",
    "\n",
    "        print(layer_name, \"sub-layer 2\")\n",
    "        # 두번째 Encoder-Decoder Attention은 padding masking을 수행한다.\n",
    "        encoder_decoder_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = encoder_decoder_attention.get_attention(query_from_decoder, key_from_encoder, value_from_encoder, mask_type='padding')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += sublayer_output_1\n",
    "        # Normalization\n",
    "        sublayer_output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        print(layer_name, \"sub-layer 3\")\n",
    "        # Feed Forward Network\n",
    "        # 입력과 출력의 크기가 보존되며, FFN의 은닉층 크기는 dff다.\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.dff, activation='relu')(sublayer_output_2)\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.d_model)(feed_forward_net)\n",
    "\n",
    "        feed_forward_net = tf.keras.layers.Dropout(rate=0.1)(feed_forward_net)\n",
    "        # Residual connection\n",
    "        feed_forward_net += sublayer_output_2\n",
    "        # Normalization\n",
    "        decoder_layer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(feed_forward_net)\n",
    "\n",
    "        return tf.keras.Model(inputs=[decoder_input, encoder_output], outputs=decoder_layer_output, name=layer_name)\n",
    "\n",
    "\n",
    "    def get_decoder(self):\n",
    "        \n",
    "        #Input 4개 : 디코더 입력, 인코더 출력, Look-ahead mask, padding mask\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_inputs\")\n",
    "        encoder_output = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_outputs\")\n",
    "\n",
    "        # Positional Encoding\n",
    "        input = PositionalEncoding(self.N, self.d_model)(decoder_input)\n",
    "        \n",
    "        # Decoder Layer 쌓기\n",
    "        # layer_num 만큼 decoder layer를 쌓는다\n",
    "        for idx in range(self.layer_num):\n",
    "            input = decoder_output = self.stack_decode_layer(layer_name=\"decoder_layer_{}\".format(idx))(inputs=[input, encoder_output])\n",
    "\n",
    "        return tf.keras.Model(inputs=[decoder_input, encoder_output], outputs=decoder_output, name=\"Decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, vocab_size, layer_num, dff, d_model, num_heads):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    encoder input : 인코더의 입력은, 요약하지 않은 문장을 토큰화하여 임베딩한 벡터들.\n",
    "    decoder input : 디코더의 입력은 요약된 문장을 토큰화하여 임베딩한 벡터들.\n",
    "    '''\n",
    "    def get_transformer(self):\n",
    "        \n",
    "        #Input 2개 : 인코더 입력, 디코더 입력\n",
    "        encoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_inputs\")\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_inputs\")\n",
    "\n",
    "        #인코더\n",
    "        encoder = Encoder(self.vocab_size, self.layer_num, self.dff, self.d_model, self.num_heads)\n",
    "        encoder_output = encoder.get_encoder()(inputs=[encoder_input])\n",
    "\n",
    "        #디코더\n",
    "        decoder_output = decoder.get_decoder()(inputs=[decoder_input, encoder_output])\n",
    "\n",
    "        '''\n",
    "        디코더에서는 인코더의 행렬과 디코더의 입력을 통해 다음 단어를 예측한다.\n",
    "        디코더의 출력은 임베딩 벡터의 개수 vocab size의 크기를 가지며, 확률 값을 가진다.\n",
    "        '''\n",
    "        # 단어 예측을 위한 출력층\n",
    "        output = tf.keras.layers.Dense(units=self.vocab_size, name=\"Output\")(decoder_output)\n",
    "\n",
    "        return tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=output, name=\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRate(tf.keras.callbacks.LearningRateScheduler):\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step_num):\n",
    "        min_val = tf.math.minimum(step_num ** -0.5,\n",
    "                                  step_num * (self.warmup_steps ** -1.5))\n",
    "        lrate = (self.d_model ** -0.5) * min_val\n",
    "        return lrate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
