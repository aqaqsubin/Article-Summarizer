{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pjJ1JudwVof4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzdjxWcyM_Br"
   },
   "source": [
    "### **Positional Encoding**  \n",
    "\n",
    "Transformer는 입력 시퀀스를 병렬적으로 처리하기 때문에, 입력 데이터는 위치정보를 포함하고 있어야 한다.  \n",
    "Positional Encoding은 각 입베딩 벡터에 위치 정보를 더하는 과정이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OFdpr3UoVtQF"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, N, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.get_positional_encoding(N, d_model)\n",
    "    \n",
    "    def get_positional_encoding(self, N, d_model):\n",
    "        \n",
    "        def get_pos_matrix(pos, i, d_model):\n",
    "            pos_matrix = pos / tf.math.pow(10000, i / tf.cast(d_model, tf.float32))\n",
    "            pos_matrix = pos_matrix.numpy()\n",
    "\n",
    "            pos_matrix[:,0::2] = tf.math.sin(pos_matrix[:,0::2])\n",
    "            pos_matrix[:,1::2] = tf.math.cos(pos_matrix[:,1::2])\n",
    "\n",
    "            return pos_matrix\n",
    "\n",
    "        pos_encoding = get_pos_matrix(pos=tf.range(N, dtype=tf.float32)[:, tf.newaxis], i=tf.range(d_model, dtype=tf.float32)[tf.newaxis,:], d_model=d_model)\n",
    "   \n",
    "        print(\"Positional Encoding\", pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding[:,:], tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_H8jIomNdFr"
   },
   "source": [
    "아래는 50개의 임베딩 벡터, 각 임베딩 벡터의 차원이 128일 때의 포지셔널 인코딩 행렬을 시각화한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "VMQ7cAQgYxnZ",
    "outputId": "6793b43c-b7f4-4f92-aea2-141581fcad07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding (50, 128)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos_encoder = PositionalEncoding(50, 128)\n",
    "\n",
    "plt.pcolormesh(pos_encoder.pos_encoding, cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 128))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QemGZa2VN3j2"
   },
   "source": [
    "### **Padding Mask 생성**  \n",
    "softmax 함수에 잘못된 값이 들어가는 경우에, 이 값을 연산에서 제외하기 위해 Padding Mask 단계를 거친다.  \n",
    "mask에 -1e9 값을 곱해 Attention Score 행렬에 더한다.  \n",
    "\n",
    "> 잘못된 값이란, 입력된 문장 행렬에서 <*PAD*>를 의미한다.  \n",
    "> <*PAD*> 값을 의미하는 0을 찾아 마스킹한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eR7pnTHdLC6A"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(multiple_qk):\n",
    "    mask = tf.cast(tf.math.equal(multiple_qk,0), tf.float32)\n",
    "    return mask * -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoJkfOmhMO6q",
    "outputId": "00ddf6e6-b68a-4922-c5a7-15567d14f589"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3, 2), dtype=float32, numpy=\n",
       "array([[[-1.e+09, -0.e+00],\n",
       "        [-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_k = tf.constant([[[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]]], dtype=tf.float32) \n",
    "tmp = tf.constant(np.arange(4*3*2).reshape((4,3,2)), dtype=tf.float32) \n",
    "create_padding_mask(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh5WP0R5PIFV"
   },
   "source": [
    "### **Scaled Dot-product Attention**  \n",
    "Dot-product Attention과 유사하나, k 벡터의 차원 값인 d_k의 루트 값을 나눠 scaling하는 과정이 추가되었다.\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q*K^T)*V               # Dot-product Attention\n",
    "Attention(Q, K, V) = softmax((Q*K^T)/sqrt(d_k))*V   # Scaled Dot-product Attention\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRTr2N_fstpi"
   },
   "source": [
    "Encoder Self Attention  \n",
    "\n",
    "```\n",
    "Q size (batch_size, enc_seq, d_model)  \n",
    "K size (batch_size, enc_seq, d_model)  \n",
    "V size (batch_size, enc_seq, d_model)  \n",
    "```\n",
    "\n",
    "Decoder Self Attention  \n",
    "```\n",
    "Q size (batch_size, dec_seq, d_model)  \n",
    "K size (batch_size, dec_seq, d_model)  \n",
    "V size (batch_size, dec_seq, d_model)  \n",
    "```\n",
    "\n",
    "Decoder Attention  \n",
    "```\n",
    "Q size (batch_size, enc_seq, d_model)  \n",
    "K size (batch_size, dec_seq, d_model)  \n",
    "V size (batch_size, dec_seq, d_model)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PvHLUf87dsdi"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask_type):\n",
    "\n",
    "    # Attention Score : Q * K^T\n",
    "    attention_score_matrix = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # Scaling : Divide by sqrt(d_k)\n",
    "    d_k = tf.cast(key.shape[-1], tf.float32)\n",
    "    scaled_matrix = attention_score_matrix / tf.math.sqrt(d_k)\n",
    "\n",
    "    # Padding Mask or Look-Ahead Mask\n",
    "    if mask_type is 'padding':\n",
    "        scaled_matrix += create_padding_mask(scaled_matrix)\n",
    "    elif mask_type is 'look_ahead':\n",
    "        scaled_matrix += create_look_ahead_mask(scaled_matrix)\n",
    "\n",
    "    # Softmax fuction\n",
    "    attention_weights = tf.nn.softmax(scaled_matrix, axis=-1) \n",
    "\n",
    "    # Weighted Sum : multiply V matrix\n",
    "    attention_value = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return attention_value, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ_dsk3oQa7C"
   },
   "source": [
    "아래는 `temp_k`, `temp_v`, `temp_q`가 입력으로 주어졌을 때 Attention Value와 Attention Distribution 을 구한 결과이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VWRxzKzRBBts"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  \n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  \n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSTH7_kEBEMm",
    "outputId": "0a127cb5-ca79-4669-b8a4-eca428571dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Distribution : [[0. 1. 0. 0.]]\n",
      "Attention Value : [[10.  0.]]\n"
     ]
    }
   ],
   "source": [
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, 'padding')\n",
    "print(\"Attention Distribution : {}\".format(temp_attn)) \n",
    "print(\"Attention Value : {}\".format(temp_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj_Z5yUmPcR9"
   },
   "source": [
    "### **Muti-Head Attention**  \n",
    "한번의 Attention을 수행하는 것보다, 여러번의 Attention을 병렬적으로 수행하는 것이 효율적이다.  \n",
    "각 Attention을 수행한 결과로부터 각기 다른 관점의 정보를 수집할 수 있다.  \n",
    "\n",
    "입력된 `query`, `key`, `value` 행렬 값이 각각 `(1, 1, 16)`, `(1, 4, 16)`, `(1, 4, 16)`이고, `d_model`이 16, `num_heads`가 4이라고 가정하자.  \n",
    "이를 `num_heads`로 분할하여 각각 Attention을 적용하기 위해 입력된 행렬을 다음과 같이 분할한다.\n",
    "\n",
    "```\n",
    "(batch_size, num_heads, 입력 시퀀스 개수, d_model / num_heads)\n",
    "```  \n",
    "```\n",
    "query.shape   # (1, 4, 1, 4)\n",
    "key.shape     # (1, 4, 4, 4)\n",
    "value.shape   # (1, 4, 4, 4)\n",
    "```\n",
    " \n",
    "**batch size**  \n",
    "위 예제에서 batch_size는 1이다.\n",
    "batch size는 모델 훈련에서의 1 batch 당 사용되는 token의 수를 의미한다.\n",
    "\n",
    "위 query, key, value Tensor에 scaled dot-product attention을 적용하면  \n",
    "`q vector=(1, 4)`, `k vector=(4, 4)`, `v vector= (4, 4)`에 대해 4번의 Attention을 각각 적용한 것과 동일하다.\n",
    "\n",
    "이 후에 `num_heads`로 분할되어 있던 행렬을 이어 `(batch_size, 입력 시퀀스 개수, d_model)`로 변환한다. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TzymH5FDSehq"
   },
   "outputs": [],
   "source": [
    "def printShape(Q, K, V, status):\n",
    "    print(\"[{status}] Q shape : {q}, K shape : {k}, V shape : {v}\\n\".format(status=status, q=Q.shape, k=K.shape, v=V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W_Il_a10PbuG"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"depth가 정수 형식이 아닙니다.\"\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        # Dense 층의 출력 차원은 d_model\n",
    "        self.WQ = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WK = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WV = tf.keras.layers.Dense(units=self.d_model)\n",
    "        self.WO = tf.keras.layers.Dense(units=self.d_model)\n",
    "\n",
    "    \n",
    "    def get_attention(self, query, key, value, mask_type=None):\n",
    "\n",
    "        printShape(query, key, value, \"Input\")\n",
    "        \n",
    "        def split_sequences(batch_size, num_heads, d_model, query, key, value):\n",
    "            Q_list = tf.reshape(query, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "            K_list = tf.reshape(key, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "            V_list = tf.reshape(value, (batch_size, -1, num_heads, d_model // num_heads))\n",
    "\n",
    "            return tf.transpose(Q_list, perm=[0, 2, 1, 3]), tf.transpose(K_list, perm=[0, 2, 1, 3]), tf.transpose(V_list, perm=[0, 2, 1, 3])\n",
    "            \n",
    "        # 현재 batch_size는 1이다.\n",
    "        # 모델 훈련에서의 batch 당 token의 수를 의미한다.\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q*W^Q : Dense 층 구성\n",
    "        q_WQ = self.WQ(query)\n",
    "        k_WK = self.WK(key)\n",
    "        v_WV = self.WV(value)\n",
    "        printShape(q_WQ, k_WK, v_WV, \"Dense\")\n",
    "\n",
    "        # num_heads로 입력 행렬 분할\n",
    "        # (batch_size, 입력 시퀀스 개수, d_model) -> (batch_size, num_heads, 입력 시퀀스 개수, d_model/num_heads)\n",
    "        Q_list, K_list, V_list = split_sequences(batch_size, self.num_heads, self.d_model, q_WQ, k_WK, v_WV)\n",
    "        printShape(Q_list, K_list, V_list, \"Splited\")\n",
    "\n",
    "        # Attention value \n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q_list, K_list, V_list, mask_type)\n",
    "        \n",
    "        # head를 연결하기 위한 Tensor shape 조정\n",
    "        # (batch_size, num_heads, 입력 시퀀스 개수, d_model/num_heads) -> (batch_size, 입력 시퀀스 개수, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # head 연결\n",
    "        # (batch_size, 입력 시퀀스 개수, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))\n",
    "\n",
    "        # Multi-Head 최종 결과 값\n",
    "        result = self.WO(concat_attention)\n",
    "\n",
    "        return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-UlQGW7j1xv",
    "outputId": "75eb4ee7-30f3-40ca-b8c6-ae6332055d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input] Q shape : (1, 16), K shape : (4, 16), V shape : (4, 32)\n",
      "\n",
      "[Dense] Q shape : (1, 16), K shape : (4, 16), V shape : (4, 16)\n",
      "\n",
      "[Splited] Q shape : (1, 4, 1, 4), K shape : (1, 4, 4, 4), V shape : (1, 4, 4, 4)\n",
      "\n",
      "Multi-Head Attention Output : [[[ -44.789627 -256.8918   -106.99986   100.81695   112.0555\n",
      "   -115.00504   124.938965 -200.9192   -107.952415  -58.22763\n",
      "   -102.496216 -167.80928   105.39375   194.37411   -29.439121\n",
      "   -277.7269  ]]]\n",
      "Attention Output shape : (1, 1, 16)\n"
     ]
    }
   ],
   "source": [
    "temp_k = tf.constant([[10,0,0,10,0,0,10,0,0,10,0,0,10,0,0,1],\n",
    "                      [0,10,0,0,10,0,0,10,0,0,10,0,0,10,0,1],\n",
    "                      [0,0,10,0,0,10,0,0,10,0,0,10,0,0,10,1],\n",
    "                      [0,0,10,0,0,10,0,0,10,0,0,10,0,0,10,1]], dtype=tf.float32)  \n",
    "\n",
    "temp_v = tf.constant([[   1,0,    2,0,    3,0,    4,0,    5,0,    6,0,    7,0,    1,0,    1,0,    1,0,    1,0,    1,0,    1,0,    1,0,    1,0,    1,0],\n",
    "                      [  12,0,   10,0,   10,0,   10,0,   11,0,   10,0,   13,0,   10,0,   10,0,   10,0,   10,0,   10,0,   10,0,   10,0,   10,0,   10,0],\n",
    "                      [ 123,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5,  100,5],\n",
    "                      [1111,6, 1320,6, 1034,6, 1000,6, 1019,6, 1110,6, 1031,6, 1330,6, 1032,6, 1000,6, 1000,6, 1000,6, 1000,6, 1000,6, 1000,6, 1000,6]], dtype=tf.float32)  \n",
    "\n",
    "temp_q = tf.constant([[0, 10, 0,0, 10, 0,0, 10, 0,0, 10, 0,0, 10, 0,1]], dtype=tf.float32)\n",
    "\n",
    "attention = MultiHeadAttention(4, 16)\n",
    "result = attention.get_attention(temp_q, temp_k, temp_v, None)\n",
    "print(\"Multi-Head Attention Output : {}\".format(result))\n",
    "print(\"Attention Output shape : {}\".format(result.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAkt3RIi4XyF"
   },
   "source": [
    "### **Encoder**  \n",
    "`class Encoder`는 `layer_num`만큼 인코더 층을 쌓은 객체이다.  \n",
    "각 layer는 keras.Model을 반환하는데, 입력 값으로 `inputs`의 크기를 가지고  \n",
    "출력 값은 `inputs`에 대한 함수 수행 결과를 반환한다.  \n",
    "\n",
    "각 encoder layer에서는 Self-Attention, Feed Forward Network를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jUzsrMluU3Xg"
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, N, layer_num, dff, d_model, num_heads, dropout=None):\n",
    "        self.N = N\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def stack_encode_layer(self, layer_name):\n",
    "        \n",
    "        # Input 1개  : 인코더 입력\n",
    "        inputs = tf.keras.Input(shape=(None, self.d_model), name=\"encode_inputs\")\n",
    "\n",
    "        print(layer_name, \"sub-layer 1\")\n",
    "        # encoder의 self attention은 query, key, value가 모두 입력 문장의 단어 벡터를 의미한다.\n",
    "        # query = key = value\n",
    "        query = key = value = inputs\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        multi_head_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = multi_head_attention.get_attention(query, key, value, mask_type='padding')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += inputs\n",
    "        # Normalization\n",
    "        sublayer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        print(layer_name, \"sub-layer 2\")\n",
    "        # Feed Forward Network\n",
    "        # 입력과 출력의 크기가 보존되며, FFN의 은닉층 크기는 dff다.\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.dff, activation='relu')(sublayer_output)\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.d_model)(feed_forward_net)\n",
    "        \n",
    "        feed_forward_net = tf.keras.layers.Dropout(rate=0.1)(feed_forward_net)\n",
    "        # Residual connection\n",
    "        feed_forward_net += sublayer_output\n",
    "        # Normalization\n",
    "        encoder_layer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(feed_forward_net)\n",
    "\n",
    "        return tf.keras.Model(inputs=[inputs], outputs=encoder_layer_output, name=layer_name)\n",
    "\n",
    "    def get_encoder(self):\n",
    "\n",
    "        inputs = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_inputs\")\n",
    "\n",
    "        # Positional Encoding\n",
    "        encoder_input = PositionalEncoding(self.N, self.d_model)(inputs)\n",
    "        \n",
    "        # Encoder Layer 쌓기\n",
    "        # layer_num 만큼 encoder layer를 쌓는다\n",
    "        for idx in range(self.layer_num):\n",
    "            encoder_input = encoder_output = self.stack_encode_layer(layer_name=\"encoder_layer_{}\".format(idx))(inputs=[encoder_input])\n",
    "\n",
    "        return tf.keras.Model(inputs=[inputs], outputs=encoder_output, name=\"Encoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zlon9KzsdvYv"
   },
   "source": [
    "### **Look-Ahead Mask 생성**  \n",
    "Transformer의 디코더는 순차적인 입력이 아니라 문장 행렬로 한꺼번에 입력되기 때문에,  \n",
    "현재 단어를 예측할 때 미래 시점의 단어가 개입되는 문제가 발생한다.  \n",
    "\n",
    "이를 방지하기 위해, 현재보다 이후 시점의 단어는 마스킹하여 미리보기를 방지한다.  \n",
    "또한 <*PAD*> 값을 포함하지 않도록 Pad Masking도 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "krbe9IPReg_C"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(attention_score_matrix):\n",
    "    N = tf.shape(attention_score_matrix)[1]\n",
    "\n",
    "    mask = tf.ones(shape=(N, N), dtype=tf.float32)\n",
    "    #mask = tf.experimental.numpy.triu(mask, 1) \n",
    "    \n",
    "    mask = 1 - tf.linalg.band_part(mask, -1, 0)\n",
    "    mask = mask[tf.newaxis, :, :] * -1e9\n",
    "\n",
    "    pad_mask = create_padding_mask(attention_score_matrix)\n",
    "    return tf.minimum(mask, pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQMXS89DkZmL",
    "outputId": "968d6f0d-bf51-429b-9c78-be16c687fdf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4, 4), dtype=float32, numpy=\n",
       "array([[[-1.e+09, -1.e+09, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -1.e+09, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -1.e+09, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00]],\n",
       "\n",
       "       [[-0.e+00, -1.e+09, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -1.e+09, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tf.constant(np.arange(4*4*4).reshape(4,4,4))\n",
    "create_look_ahead_mask(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIPePp4o6G2m"
   },
   "source": [
    "### **Decoder**  \n",
    "`class Decoder` 또한 `layer_num`만큼 층을 쌓은 객체이다.  \n",
    "입력 값은 `d_model`의 차원을 가지며, 출력 값으로 `inputs`에 대한 함수 수행 결과를 반환한다.  \n",
    "\n",
    "각 decoder layer에서는 Masked Self-Attention, Encoder-Decoder, Feed Forward Network를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "plUZCVk0uor6"
   },
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, N, layer_num, dff, d_model, num_heads, dropout=None):\n",
    "        self.N = N\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def stack_decode_layer(self, layer_name):\n",
    "        print(layer_name, \"sub-layer 1\")\n",
    "        \n",
    "        #Input 2개 : 디코더 입력, 인코더 출력\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_layer_input\")\n",
    "        encoder_output = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_output\")\n",
    "\n",
    "        # Masked Multi-Head Self Attention\n",
    "        # 디코더의 Self Attention에서 query, key, value의 출처는 디코더 입력이다.\n",
    "        query = key = value = decoder_input\n",
    "\n",
    "        self_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = self_attention.get_attention(query, key, value, mask_type='look_ahead')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += decoder_input\n",
    "        # Normalization\n",
    "        sublayer_output_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        # Multi-Head Encoder-Decoder Attention\n",
    "        # 디코더 Encoder-Decoder Attention의 입력 중 Q는 디코더 sub-layer의 출력이고, K,V는 인코더의 출력이다.\n",
    "        key_from_encoder = value_from_encoder = encoder_output\n",
    "        query_from_decoder = sublayer_output_1\n",
    "\n",
    "        print(layer_name, \"sub-layer 2\")\n",
    "        # 두번째 Encoder-Decoder Attention은 padding masking을 수행한다.\n",
    "        encoder_decoder_attention = MultiHeadAttention(self.num_heads, self.d_model)\n",
    "        attention_value = encoder_decoder_attention.get_attention(query_from_decoder, key_from_encoder, value_from_encoder, mask_type='padding')\n",
    "\n",
    "        attention_value = tf.keras.layers.Dropout(rate=0.1)(attention_value)\n",
    "        # Residual connection\n",
    "        attention_value += sublayer_output_1\n",
    "        # Normalization\n",
    "        sublayer_output_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_value)\n",
    "\n",
    "        print(layer_name, \"sub-layer 3\")\n",
    "        # Feed Forward Network\n",
    "        # 입력과 출력의 크기가 보존되며, FFN의 은닉층 크기는 dff다.\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.dff, activation='relu')(sublayer_output_2)\n",
    "        feed_forward_net = tf.keras.layers.Dense(units=self.d_model)(feed_forward_net)\n",
    "\n",
    "        feed_forward_net = tf.keras.layers.Dropout(rate=0.1)(feed_forward_net)\n",
    "        # Residual connection\n",
    "        feed_forward_net += sublayer_output_2\n",
    "        # Normalization\n",
    "        decoder_layer_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(feed_forward_net)\n",
    "\n",
    "        return tf.keras.Model(inputs=[decoder_input, encoder_output], outputs=decoder_layer_output, name=layer_name)\n",
    "\n",
    "\n",
    "    def get_decoder(self):\n",
    "        \n",
    "        #Input 4개 : 디코더 입력, 인코더 출력, Look-ahead mask, padding mask\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_inputs\")\n",
    "        encoder_output = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_outputs\")\n",
    "\n",
    "        # Positional Encoding\n",
    "        input = PositionalEncoding(self.N, self.d_model)(decoder_input)\n",
    "        \n",
    "        # Decoder Layer 쌓기\n",
    "        # layer_num 만큼 decoder layer를 쌓는다\n",
    "        for idx in range(self.layer_num):\n",
    "            input = decoder_output = self.stack_decode_layer(layer_name=\"decoder_layer_{}\".format(idx))(inputs=[input, encoder_output])\n",
    "\n",
    "        return tf.keras.Model(inputs=[decoder_input, encoder_output], outputs=decoder_output, name=\"Decoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhHDp8Go6nVy"
   },
   "source": [
    "### **Transformer**  \n",
    "`class Transformer`는 트랜스포머 모델의 객체이다.  \n",
    "`layer_num` 만큼 인코더와 디코더를 쌓고, 인코더 결과 값을 디코더에 연결한다.  \n",
    "\n",
    "디코더의 출력 값을 통해, 다음 단어를 예측한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UaatIZbZZFY6"
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self, vocab_size, layer_num, dff, d_model, num_heads):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layer_num = layer_num\n",
    "\n",
    "        self.dff = dff\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    '''\n",
    "    encoder input : 인코더의 입력은, 요약하지 않은 문장을 토큰화하여 임베딩한 벡터들.\n",
    "    decoder input : 디코더의 입력은 요약된 문장을 토큰화하여 임베딩한 벡터들.\n",
    "    '''\n",
    "    def get_transformer(self):\n",
    "        \n",
    "        #Input 2개 : 인코더 입력, 디코더 입력\n",
    "        encoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"encoder_inputs\")\n",
    "        decoder_input = tf.keras.Input(shape=(None, self.d_model), name=\"decoder_inputs\")\n",
    "\n",
    "        #인코더\n",
    "        encoder = Encoder(self.vocab_size, self.layer_num, self.dff, self.d_model, self.num_heads)\n",
    "        encoder_output = encoder.get_encoder()(inputs=[encoder_input])\n",
    "\n",
    "        #디코더\n",
    "        decoder = Decoder(self.vocab_size, self.layer_num, self.dff, self.d_model, self.num_heads)\n",
    "        decoder_output = decoder.get_decoder()(inputs=[decoder_input, encoder_output])\n",
    "\n",
    "        '''\n",
    "        디코더에서는 인코더의 행렬과 디코더의 입력을 통해 다음 단어를 예측한다.\n",
    "        디코더의 출력은 임베딩 벡터의 개수 vocab size의 크기를 가지며, 확률 값을 가진다.\n",
    "        '''\n",
    "        # 단어 예측을 위한 출력층\n",
    "        output = tf.keras.layers.Dense(units=self.vocab_size, name=\"Output\")(decoder_output)\n",
    "\n",
    "        return tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=output, name=\"Transformer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5E5NQniiogV"
   },
   "source": [
    "```\n",
    "vocab_size  # 입력 시퀀스 길이\n",
    "layer_num   # 인코더/디코더 층\n",
    "dff         # FFN의 셀 개수\n",
    "num_heads   # Multi-head Attention 수행 시 head 개수\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "T6imsMfd8hyw"
   },
   "outputs": [],
   "source": [
    "D_MODEL = 256\n",
    "LAYER_NUM = 6\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "VOCAB_SIZE = 100000\n",
    "\n",
    "WARMUP_STEPS = 50\n",
    "EPOCHS = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiPQhYFn3iY2"
   },
   "source": [
    "### **Learning Rate**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vsWa8ZrE4s1K"
   },
   "outputs": [],
   "source": [
    "class LearningRate(tf.keras.callbacks.LearningRateScheduler):\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step_num):\n",
    "        min_val = tf.math.minimum(step_num ** -0.5,\n",
    "                                  step_num * (self.warmup_steps ** -1.5))\n",
    "        lrate = (self.d_model ** -0.5) * min_val\n",
    "        return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kK_y0qnl63JE"
   },
   "outputs": [],
   "source": [
    "lrate_scheduler = LearningRate(d_model=D_MODEL, warmup_steps=WARMUP_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OTfrw833G_P"
   },
   "source": [
    "### **Optimizer** : Adam Optimizer  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MiohcEbl5u1n"
   },
   "outputs": [],
   "source": [
    "beta_1 = 0.9  \n",
    "beta_2 = 0.98\n",
    "epsilon = 10 ** -9\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lrate_scheduler, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTqzPB875yMt"
   },
   "source": [
    "### **Loss Function** : Cross Entropy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9iBfJdq-aTu",
    "outputId": "dfa80657-0f3d-454b-84a2-b37a4b405b39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding (100000, 256)\n",
      "encoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_0 sub-layer 2\n",
      "encoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_1 sub-layer 2\n",
      "encoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_2 sub-layer 2\n",
      "encoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_3 sub-layer 2\n",
      "encoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_4 sub-layer 2\n",
      "encoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "encoder_layer_5 sub-layer 2\n",
      "Positional Encoding (100000, 256)\n",
      "decoder_layer_0 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_0 sub-layer 3\n",
      "decoder_layer_1 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_1 sub-layer 3\n",
      "decoder_layer_2 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_2 sub-layer 3\n",
      "decoder_layer_3 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_3 sub-layer 3\n",
      "decoder_layer_4 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_4 sub-layer 3\n",
      "decoder_layer_5 sub-layer 1\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 2\n",
      "[Input] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Dense] Q shape : (None, None, 256), K shape : (None, None, 256), V shape : (None, None, 256)\n",
      "\n",
      "[Splited] Q shape : (None, 8, None, 32), K shape : (None, 8, None, 32), V shape : (None, 8, None, 32)\n",
      "\n",
      "decoder_layer_5 sub-layer 3\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    layer_num=LAYER_NUM,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS).get_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "HLDKkcw8AJQr",
    "outputId": "46d9de38-cb19-445a-cbce-b5701789a7d7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "YCJGdg9j-qOL"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oufUXi8AbHw"
   },
   "source": [
    "### **Load Dataset**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/data/TestDir\"\n",
    "\n",
    "DATA_BASE_DIR = os.path.join(BASE_DIR, 'sample_articles')\n",
    "PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Preprocessed-Data\")\n",
    "SUMMARY_PREPROCESSED_PATH = os.path.join(DATA_BASE_DIR,\"Summary-Preprocessed-Data\")\n",
    "\n",
    "MODEL_BASE_DIR = os.path.join(BASE_DIR, 'articleSummary-Jupyter')\n",
    "glove_model_path = os.path.join(MODEL_BASE_DIR, 'glove.model')\n",
    "corpus_model_path = os.path.join(MODEL_BASE_DIR, 'corpus.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "glove = Glove.load(glove_model_path)\n",
    "corpus = Corpus.load(corpus_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 된 기사를 통해 토큰화 및 정수 인코딩 수행  \n",
    "\n",
    "```\n",
    "\"국회 정기 국회 마지막 날 정국 전운 감돌 있\"      # 전처리 된 기사\n",
    "\n",
    "[32, 342, 273, 280, 129, 222, 34, 55, 3] # 정수 인코딩 수행한 문장\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RawTextReader:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.rgxSplitter = re.compile(\"/n\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath, encoding='utf-8'):\n",
    "            ch = self.rgxSplitter.split(line)\n",
    "            for s in ch:\n",
    "                yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerEncoder:\n",
    "    def __init__(self, filepath, corpus):\n",
    "        self.filepath = filepath\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def get_encoded_vec_list(self):\n",
    "        media_list = os.listdir(self.filepath)\n",
    "        \n",
    "        encoded_vec_list = []\n",
    "        for media_name in media_list:\n",
    "    \n",
    "            media_path = os.path.join(self.filepath, media_name)\n",
    "            article_list = os.listdir(media_path)\n",
    "            \n",
    "            for article_name in article_list:\n",
    "                \n",
    "                reader = RawTextReader(os.path.join(media_path, article_name)) \n",
    "                content = list(filter(None, reader))\n",
    "                corpus\n",
    "                vec = [corpus.dictionary[token] for sent in content for token in sent.split() if token in corpus.dictionary]\n",
    "                encoded_vec_list.append(vec)\n",
    "                \n",
    "\n",
    "                return encoded_vec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target data는 train data로부터 추출된 문장이므로, target vocabulary와 train vocabulary는 동일하다.  \n",
    "\n",
    "아래는 PREPROCESSED_PATH 내 텍스트를 이용해 train data 구성하는 내용이다.  \n",
    "\n",
    "\n",
    "```\n",
    "Encoder Input Sequence Number : 기사의 개수 (약 100,000개)\n",
    "Decoder Input Sequence Number : 기사의 요약문 개수 (약 100,000개)\n",
    "\n",
    "D_MODEL : 임베딩 벡터 차원 (256 차원)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder = IntegerEncoder(PREPROCESSED_PATH, corpus)\n",
    "train_encoded_matrix = train_encoder.get_encoded_vec_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY_PREPROCESSED_PATH 내 텍스트를 이용해 target data 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = IntegerEncoder(SUMMARY_PREPROCESSED_PATH, corpus)\n",
    "target_encoded_matrix = target_encoder.get_encoded_vec_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vPfUMKV-_8E"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer-without-mask-layer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
