남성 위주 데이터, 인간의 편견 학습… AI도 공정성 의문
“차라리 기계에 맡기는 게 낫겠다.”특정 기업 채용에서 불공정성이 불거지거나 경찰·검찰 수사가 편파적이라고 느낄 때 또는 판결이 판사의 가치지향 등에 따라 편향적으로 나온다고 생각할 때 사람들은 이렇게 말한다. 인공지능(AI)이 가치중립적일 것이라는 기대가 깔려 있다.하지만 최근까지 나온 사례들만 보면 AI에 완벽한 공정성을 바라는 건 회의적이다. 결국 AI도 이를 만든 사람이 가진 편견을 그대로 학습하기 때문이다.●아마존, 여성 구직자 차별한 ‘채용 AI ’ 폐기세계 최대 전자상거래 기업 아마존의 ‘채용 AI 폐기 사건’이 대표적인 사례다. 이 회사는 컴퓨터 프로그램을 활용해 편견 없이 최고의 인재를 뽑기 위해 2014년부터 AI 개발팀을 운영했다. AI가 구직자의 이력서를 검토해 별점을 1~5개 부여하는 방식으로 평가하고 합격자를 가려내는 방식이었다.하지만 개발 과정에서 결정적 문제가 발견됐다. AI가 여성 구직자를 차별하고 있었기 때문이다. 로이터통신 등에 따르면 이력서에 ‘여성’, ‘여성 체스 동아리 회장’ 등의 표현이 나오면 감점시켰다. 또 여대를 나온 구직자 2명의 점수도 깎았다.●과거 재판 데이터화… 소수자 등 편견 가질 것전문가들은 아마존의 채용 AI가 지난 10년간 회사에 제출된 이력서와 합격자, 이후 승진자 정보 등을 바탕으로 학습한 탓에 편견에서 자유롭지 못한 것이라고 추정한다. 아마존 같은 기술 기업들은 남성 지배적 조직이 많다. AI가 이 데이터를 바탕으로 “남성이 우리 기업에 더 적합하다”고 판단했다는 얘기다. 아마존은 이 AI를 실제 채용 때 사용하지 않고 2018년 폐기했다.수사나 형사 재판 과정에 도입될 AI도 인간이 가진 편견에서 자유롭기 어렵다. ‘유전무죄 무전유죄’ 같은 잘못된 인식도 학습될 수 있어서다.캐런 하오 MIT테크널러지리뷰 선임기자는 “(AI가 과거 수사·재판 기록 등을 토대로 학습하면) 역사적으로 수사·사법 당국의 집중 표적이 돼 온 저소득층이나 소수자들이 이후에도 범죄를 상습적으로 저지를 것이라고 이해할 수 있다”고 우려했다.유대근 기자 dynamic@seoul.co.kr▶  ▶  ▶ⓒ 서울신문(www.seoul.co.kr), 무단전재 및 재배포금지
서울신문
